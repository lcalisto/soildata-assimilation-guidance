{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This WIKI is a collaborative effort to collect and describe hands-on good practices on data assimilation and dissemination in the Soil domain, with a focus on Europe. The INSPIRE directive has been and is an important effort for standardisation in the environmental data domain, therefore this WIKI has a lot of links to INSPIRE sources. Because INSPIRE adopts industry standards, this WIKI does reference common standards from ISO, Open Geospatial Consortium, Global Soils Partnership, IANA and W3C, giving it a global relevance.</p> <p>For data assimilation 3 aspects are important:</p> <ul> <li>Data dissemination and consumption; the process of making data discoverable and available, including the actual collection of the data</li> <li>Data harmonization; the process of adopting or transforming data to a common data model</li> </ul> <p>Toner et al, 2022 identified 6 steps in a typical soil information workflow from a data producer perspective and a separate category for the data user perspective. These steps form a relevant categorisation of the articles in this wiki. Much of the articles apply to the categories <code>4) Data organisation</code> and <code>6) data and info sharing</code> and <code>7) Soil Information User Consideration</code>. We've labeled each of the articles as to which step in the workflow they apply.</p> <p></p> <p>The INSPIRE implementation rules (IR) present approaches to harmonise and publish data. There is a number of conventions, tools and practices available to faciliate this. Initially, the JRC published a series of technical guidelines (TG). These days good practice documents (GP) are prepared by the community and adopted by JRC. By following these guidelines or good practices, you are facilitated to be compliant to the INSPIRE rules according to agreed upon procedures. However, you can still publish data in alternative ways, but in that case you need to declare how you comply to the rules with your implementation.</p> <p>This wiki lists a series of options for publishing data according to the Technical Guidelines and/or the Good Practices dedicated to use cases from the Soil domain. For each option recipes on various technologies are provided. The practices are categorized at 3 levels:</p> <ul> <li>Minimal, based on a minimal effort</li> <li>Traditional, following initial technical guidelines</li> <li>Experimental, following recent and upcoming good practices</li> </ul> <p>The practices cover 7 topics.</p> <ul> <li>Identification and namespaces</li> <li>Data harmonization</li> <li>Code lists</li> <li>Metadata and Discovery</li> <li>View services</li> <li>Download services</li> <li>Quality of service</li> </ul> <p>Info</p> <p>Disclaimer: References to products and approaches are examples. We do not aim to provide a complete listing, nor endorse a specific technology or service provider. Please consult any alternative software provider to what extent INSPIRE is supported in their products. In that scenario consider to contribute your experiences to this WIKI. </p>"},{"location":"#reading-guide","title":"Reading guide","text":"<p>While reading the document you may realize; if INSPIRE offers so many options, then consuming and combining data on the client side will be a challenge, a client needs to be aware of all these practices and conventions. But consider that despite the various technical options (which could also be bridged by intermediaries) there are certain aspects that are constant in any of the implementations; any implementation covers some form of data discovery, harmonization, view and download services. But also, aspects such as identifier persistence and adoption of code lists, as we will explain.</p> <p>When selecting one of the available options, consider the following aspects:</p> <ul> <li>The minimal implementation will have limitations for end users (for example having to download the full dataset, if they are only interested in a small section). On the other hand, minimal implementations tend to be less complex in setup which makes understanding the implementation easier.</li> <li>The traditional implementation has the most active users, so dedicated documentation and tooling is available with high Technical Readiness Level (TRL). However, the technology is based on conventions of almost 20 years ago, some conventions are outdated with current IT practices.</li> <li>An experimental approach brings the risk of incomplete documentation and tools. Also there is less evidence on conformance to the directive. But it also gives opportunity to use current technologies and engage with the community to design the next iteration of INSPIRE.</li> </ul> <p>Before selecting an option evaluate the following aspects in your organization.</p> <ul> <li>What are current IT tools and conventions used in the organization to understand which of the approaches fits best with the current knowledge and experience</li> <li>Combine an implementation of INSPIRE with business cases that generate direct benefit for your organization or partners. For example, adoption of the open data directive, better documentation and reporting of service levels, improved archival of data, discoverability on search engines.</li> <li>Assess the projected audience. Verify that the complexity and nature of the implementation matches with the expectations and capabilities of that audience.</li> </ul> <p>We recommend to start with a minimal implementation and validate it with the provided compliance test tooling. From there, extend the implementation while continuing the tests with each iteration. In this scenario you are able to focus on the important aspects and prevent caveats early on in the process.</p> <p>For background reading on terminology and general principles described in this wiki (like data modelling, ontologies and the INSPIRE model), please see EJP Soil Deliverable 6.1 chapter 3.</p>"},{"location":"QOS/","title":"Overview Quality of Service","text":"<p>Via the quality of service conventions data providers report about the quality of their services. Aspects which are monitored are:</p> <ul> <li>Availability (how long was a service down or offline in a period)</li> <li>Performance and capacity (a period in which performance and capacity requirements are not met, is considered downtime)</li> <li>Usage (how much is the service used)</li> </ul> <p>Measuring and reporting about Quality of Service is an aspect of step <code>7) Soil Information User Consideration</code> in the soil information workflow.</p> <p>There is no minimal, traditional or experimental approach on how to measure these quality indicators. Consult your IT department or hosting company which tools they are used to work with. Confirm with them to extend and/or share these measurements for the requested parameters.</p> <p>To know the availability of a service, it requires to permanently monitor the availability of the service (excluding the maintenance windows). A basic alive-test every 5 minutes is sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom. A special mention for the Python based GeoHealthCheck package, which includes the capability on WMS/WFS services to drill down to the data level starting from the GetCapabilities operation.</p> <p>To know the capacity and performance of the service it is optimal to perform some load tests prior to moving to production. A common mistake is to provide a WMS service to a big vector dataset. When requesting that dataset on a national level, the server runs into problems drawing all the features at once. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like Kibana and evaluate the number of requests exceeding the limits.</p> <p>To capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk or AW stats. Defining rules to extract the requested layer name from a WMS request is useful. Mind that not all requests are GET requests, some WFS requests use POST, which may need some configuration on the webserver to enable POST parameter logging. Make sure the logging includes the 'Referer' and 'User-agent' parameters, which allows to differentiate types of uses. Finally consider there is a GDPR privacy aspect to collecting access logs. Consider to exclude the IP address of the user and define a maximal retention for access logs.</p>"},{"location":"codelists/","title":"Overview Code Lists","text":"<p>Adoption of common code lists is an important aspect of data harmonization. For INSPIRE the INSPIRE registry is the source of common code lists. Other common codelists relevant to the soil domain are available in FAO Agrovoc, GEMET, OGC definition server, ISO TC211, and GLosis. At a national level some countries have implemented a national repository for common national codelists which may be relevant (either as a source or as a target, to publish an extended list).</p> <p>Adoption of common code lists is an aspect of step <code>4) data organization</code> in the soil information workflow, although it could also impact step <code>1) data collection</code>.</p> <p>The adoption of code lists has three aspects:</p> <ul> <li>Inventarisation of the code lists used in the source model</li> <li>Evaluation of the differences between local and common code lists</li> <li>Some code lists are a full match</li> <li>Some code lists need to be extended, or values mapped</li> <li>Some local code lists do not (yet) have a common code list available</li> <li>In cases where the common code list cannot (fully) be adopted, the code list needs to be published in a local repository</li> </ul> <p>Adoption of a dedicated codelist is relevant for example for Soil classification. Many of the national soil classification systems have much more detail than the World Reference Base, as suggested to be used by the TG Soil.</p> Please note that the harmonization meant here is harmonization of the description of the data, for example describing a soil observation of pH KCl with dilution 1:10 in the same way across Europe. The harmonization of the data itself, for example transforming pH KCl values to pH H2O values, is a separate step and not described in this wiki. More information on that harmonization can be found in D6.1 chapter 3.5 page 122. <p>The soil theme has a large number of code lists, ranging from soil type to ranges of grain size. Many code lists originate from the FAO soil classification and are published in the INSPIRE registry.</p> <p>If you missed the 2022 EJP Training on Soil data good practices, you can still have a look at a presentation about codelists.</p> <p>Implementation options for managing and publishing a code list:</p>"},{"location":"codelists/#minimal","title":"Minimal","text":"<p>The most basic form of publishing an alternative or extended code list is to place a code list file on a web location and reference values in it as http://example.com/codelist.xml#concept (see for example http://schemas.opengis.net/iso/19139/20070417/resources/Codelist/gmxCodelists.xml)</p> Cookbook Software Description Code list as iso19135 Publish a xml file on a web location"},{"location":"codelists/#traditional","title":"Traditional","text":"<p>Extended code lists can be published in a local or national instance of the Re3gistry software. This open source project is hosted by JRC to facilitate the INSPIRE registry.</p> Cookbook Software Description Code list in Re3gistry Re3gistry Publish a codelist in Re3gistry"},{"location":"codelists/#experimental","title":"Experimental","text":"<p>A standard for the definition of code lists is Simple Knowledge Organization System (SKOS). Any SPARQL endpoint can be used to publish a code list based on SKOS. Software exists which facilitates the consumption of SKOS data from a SPARQL endpoint in a human friendly way. An example is Skosmos.</p> <p>A powerfull aspect of SKOS is that you can link from a concept to existing concepts in other codelists using link relations such as: sameAs, Broader, Narrower.</p> Cookbook Software Description SKOS codelist in semantic web Virtuoso Skosmos Publish a semantic web code list"},{"location":"download/","title":"Overview Download Services","text":"<p>Download services facilitate the download of vector, grid or sensor data. </p> <p>If you missed the initial EJP Training on Soil data good practices, you can still have a look at a presentation about download services as Coverage and SensorThings or a presentation on Atom/WMS/WFS.</p> <p>Setting up download services is an aspect of step <code>6) data and info sharing</code> in the soil information workflow.</p> <p>This page lists some implementation options for providing INSPIRE Download Services.</p>"},{"location":"download/#minimal","title":"Minimal","text":"<p>The INSPIRE Atom Service provides a minimal download service implementation on a series of downloadable files placed in a web accessible folder. For every file a 'dataset feed' document can be generated and linked to a service feed describing the 'service'. A metadata record points to the service feed to complete the implementation.</p> <p>Alternatively, products like GeoNetwork and Hale Connect (Annex 1) can provide an Atom interface on top of a set of registered datasets. The TG download services also provides some PHP scripts which create an Atom OpenSearch interface for a series of files.</p> Cookbook Software Description Webdav Wsgidav Setting up atom service using webdav GeoNetwork Atom GeoNetwork Atom download service from GeoNetwork"},{"location":"download/#traditional","title":"Traditional","text":"<p>Web Feature Service (WFS) and Web Coverage Service (WCS) are commonly used to download Featurecollections (vector) and Coverages (grids). Consider that most of the INSPIRE themes (including soil) require publication of hierarchical (app-schema) features, this aspect is not supported by many WFS server implementations. Some tools with this capability are listed in the table below. For WCS a good practice is available to facilitate implementation.</p> Cookbook Software Description deegree deegree Java based feature server implementations which also includes WMS, WMTS, CSW, WCS interfaces, deegree facilitates on the fly transformation from relational data as well as publication of pregenerated xml fragments. Xtraserver XtraServer / ArcGIS Xtraserver, either standalone or as INSPIRE plugin for ArcGIS, facilitates on the fly transformation of relational data to INSPIRE GML as part of the WFS service definition. The developers of Xtraserver lead the GML working group at OGC GeoServer GeoServer The app-schema plugin extends the WFS implementation with support for hierarchical features. On the fly transformation is managed from a configuration file. Marcus Sen (Onegeology) create a cookbook for this approach. Get started with Hale Connect Hale Connect Optimized for performance, stores pregeneralised xml fragments in combination with an elastic search index for filtering Coverages with rasdaman Rasdaman A Web Coverage Service implementation <p>Consider that a product advertising WFS support does not automatically qualify for INSPIRE, the product has to support hierarchical GML.</p>"},{"location":"download/#experimental","title":"Experimental","text":"<p>As described in the harmonization paragraph, Sensor Observation Service and SensorThings API offer an alternative download option for themes including much observation data, such as Soil.</p> <p>A good practice document has been adopted on the use of OGC API Features as download service. With its 20 years of history WFS and GML are out of synch with current IT practices. OGC API is a new direction of standards within OGC adopting some of the latest IT conventions, such as Open API, Rest services, JSON encodings, content negotiation, etc. The use of OGC API will increase in coming years while OGC adopts more standards and good practice documents around these standards will be written and adopted by JRC. Various products exist implementing the final and/or draft specifications.</p> <p>A SPARQL endpoint is a typical endpoint for downloading data within the semantic web. In the table below some guidance on various approaches to the semantic web.</p> <p>GraphQL is an industry standard for queries on hierarchical data using modern api concepts. GraphQL is not frequently associated to Geospatial data, but it would be a good fit for soil data.</p> Cookbook Software Description pygeoapi pygeoapi A python based open source implementation of OGC APIs including OGC API Features. Configuration is managed from a configuration file. LDProxy ldproxy A java based open source implementation of OGC APIs. Originally developed by Interactive Instruments as an easy way to consume API (proxy) on top of existing WFS. These experiments were a main driver for OGC in the direction of OGC API. Configuration is managed from a web interface. GeoServer GeoServer OGC API is a community plugin of GeoServer, it publishes an alternative endpoint for the datasets published as WFS. QGIS server QGIS QGIS Server includes the option to enable OGC API Features access to datasets published as WFS Cookbook Semantic Web Blazegraph / Coby INRAE prepared a linked data primer for semantic soil data Virtuoso &amp; skosmos Virtuoso Triple store providing a SPARQL endpoint YARRRML yarrrml Human readable declaritive mapping rules for semantic web Postgraphile Postgraphile Plugin offering graphile access to postgres databases"},{"location":"etl/","title":"Data harmonization (vector data)","text":"<p>A main aspect of INSPIRE is the harmonization of environmental data throughout Europe. Alinging the format, structure and content of your data to match a common model, so the data can be integrated with other datasets in Europe. And to describe aspects of the data which differ from the common model in such a way, that the differences can be understood by others. </p> <p>Data harmonization is an aspect of step <code>4) data organization</code> in the soil information workflow.</p> <p>An important activity related to harmonization is the adoption of INSPIRE code lists and extending those lists to capture regional conventions. The role of code lists is explained in a dedicated codelists section. </p> <p>INSPIRE datamodel extension is another aspect to consider as part of data harmonization. Harmonization should not lead to loss of data (because some data doesn't fit the target model). Instead the target model should be extended to capture these aspects. Annex D of TG Soil has a specific example on extending the model for a soil contamination use case. Options for model extensions vary per technology. Wetransform has a dedicated section on model extension on their website, based on an R&amp;D project from 2016. </p> <p>If you missed the initial EJP Training on Soil data good practices, you can still have a look at a presentation about vector data harmonization.</p> <p>This document lists various implementation options for data harmonization. </p>"},{"location":"etl/#minimal","title":"Minimal","text":"<p>The good practice on GeoPackage describes a relational database format to share harmonised data. GeoPackage is a standardised format for storing relational data by the Open Geospatial Consorium, building on the SQLite database specification. Becuase many soil data is stored in the form of relational databases, the transformation to GeoPackage is relatively easy. The transformation process could for example be triggered by a series of database queries within a GIS Desktop client such as QGIS, in Python scripts or an ETL tool such as Hale Studio or FME. </p> <p>Pro's and Con's:</p> <ul> <li>The GeoPackage format is easy to consume by average users.</li> <li>To capture the hierarchical structure of the INSPIRE datamodels, a lot of tables are needed, resulting in a complex data-model. </li> <li>The good practice is recent, so not a lot of community experience is available yet </li> <li>Users download a full dataset, no (filering) api's are defined for GeoPackages yet</li> </ul> Cookbook Software Description Glosis as a database - Harmonise soil data using GeoPackage"},{"location":"etl/#traditional","title":"Traditional","text":"<p>Tools like Hale Studio and FME are typically used to configure a conversion from data in a relational data model to data in a GML based INSPIRE model. The output is a big GML file which can be published using a WFS server or Atom download service. Below table links to detailed pages on various relevant technologies.</p> Cookbook Software Description Hale Studio Hale Studio Humboldt Alignment Editor Studio is a Desktop tool to author 'data alignments'. FME &amp; INSPIRE FME Feature Manipulation Engine is a visually oriented data integration platform <p>You may not have considered before, but consuming a rich GML is not straight forward in common GIS clients like ArcGIS or QGIS. To consume a rich GML you need software which is able to traverse xml hierarchies and links. Tools like Hale Studio can also be used to read rich GML and convert it back to a relational database. Unfortunately you can not automatically reverse an existing database to GML ETL-configuration. But you can set up a new ETL configuration to read and transform the rich GML. A recipe is available which imports INSPIRE Soil GML from the city of Berlin and converts it to a relational database.</p> <p>Alternatively some server tools offer on the fly transformation as part of the download service, the data mapping is defined within the service configuration.</p> Cookbook Software Description GeoServer GeoServer Java based server implementation deegree deegree Java based server implementation Xtraserver xtraserver Java based server implementation, also distributed as ArcGIS for INSPIRE Classic"},{"location":"etl/#experimental","title":"Experimental","text":"<p>Because the common soil datamodels substantially depend on Observations and Measurements (O&amp;M), the use of Sensor Observation Service (SOS) as a download service is a proper alternative to WFS. Various tools facilitate SOS, or its follow-up, SensorThings API. Katharina Schleidt provides some interesting work on setting up SensorThings API to provide INSPIRE data. See also her presentation about harmonization based on Coverage and Sensor from the 2022 EJP Soil training.</p> Product Software Description 52North 52 north Java based SOS and STA implementation Workshop API4INSPIRE Frost server Java based implementation of Sensorthings API istSOS tutorial - Python notebook istSOS Python based open source SOS 1.0 implementation <p>Various groups prefer to work with semantic web technology over UML/XSD to publish environmental data. Various tools exist which expose relational models as semantic graph.</p> <p>See also the presenation about semantic web (GLOSIS) from the 2022 EJP Soil training.</p> Product Software Description Publish data through semantic web Yed Coby BlazeGraph Cookbook by INRAE on publishing soild data as RDF Tutorial Ontop Exposes the content of arbitrary relational databases as knowledge graphs YARRRML RML mapper Relational to RDF mapping using RML.io EDG Quick start guides TopBraid EDG Desktop/SAAS solution to manage graphs including various data imports"},{"location":"identification/","title":"Identification, namespaces and URI strategy","text":"<p>An important aspect of publication of data on the web is universal identification of objects within the European INSPIRE infrastructure. Identifiers are constant, unique and authoritative.</p> <p>Resource identificiation is an aspect of step <code>4) data organization</code> in the soil information workflow.</p> <p>Any identifier is typically combined with a namespace for that identifier, or both aspects are combined into a single Universal Resource Identifier (URI) for the object. Namespaces need to be authoritative but should not be sensible to change. For example, a project name is not a good namespace, because the project is bound to end after a certain period. Examples of good namespaces are: w3id.org, doi.org, data.gouv.fr.</p> <p>Some countries have a registry of national namespaces. Select a namespace from that registry, or consider to add your namespace to that registry if it relates to a national application.</p>"},{"location":"identification/#minimal-implementation","title":"Minimal implementation","text":"<p>In a minimal implementation you can concatenate the database identifier, the featuretype and a namespace to create the INSPIRE identification for the object. For example:</p> <p>https://data.gouv.fr/inrae/collections/{featuretype}/items/{id}</p> <p>To facilitate users to use the INSPIRE identification to open the object in a web browser, you can set up a routing mechanism to forward the request to the location where the catalogue or feature server is located.</p> <p>An example of such a routing rule in Apache webserver:</p> <pre><code>RewriteRule\n  \"https://data.gouv.fr/inrae/collections/(.\\*)/items/(.\\*)$\"\n  \"http://data.recover.inrae.fr:8081/geoserver/vulter/wfs?typeNames=$1&amp;featureID=$2&amp;request=GetFeature\"\n</code></pre>"},{"location":"identification/#traditional","title":"Traditional","text":"<p>The Technical Guidelines have a long section on identification within INSPIRE, including dates indicating the validity of a feature. An interesting blog about the use of Namespaces and Identifiers is written by Thorsten Reitz at https://www.wetransform.to/news/2018/02/12/best-practices-for-inspire-ids/.</p>"},{"location":"identification/#experimental","title":"Experimental","text":"<p>These days the INSPIRE community recommends the use of URI's to identify things. This aspect is described in https://inspire.ec.europa.eu/implementation-identifiers-using-uris-inspire-%E2%80%93-frequently-asked-questions/59309.</p> <p>The aspect of identification is one of the major benefits of the upcoming OGC API's over traditional WMS, WFS, WCS. By design any feature, coverage, record or tile in OGC API has a unique URI, including content negotiation to be able to request the object in one of the available encodings (xml, json, html, Geopackage)</p>"},{"location":"identification/#common-mistakes","title":"Common mistakes","text":"<p>In many cases catalogue records and service definitions are populated manually in separate locations. Verify that at each location the identification and namespace of links between metadata and services are correct. Initially JRC did not have testing procedures to test these linkages. In practice a lot of these links where not correct, causing users not to be able to download a dataset from a search result in the national and INSPIRE GeoPortal.</p> <p>Integrated data and metadata platforms, such as deegree and Hale Connect, prevent this type of mistakes. A helpful tool in this area is GeoCat Bridge, which publishes data to GeoServer and metadata to GeoNetwork synchronously, ensuring that bidirectional linkage is correct.</p>"},{"location":"metadata/","title":"Overview Metadata &amp; discovery","text":"<p>Discovery of available data is important for potential users to be aware what data is available, evaluate if the data is relevant for them and how they can fetch it, or who to contact for more details. Essentially, the initial goal of any Spatial Data Infrastructure (SDI) is to describe its content. Metadata of datasets and services is described in documents, which are made accessible via a discovery service as records in a catalogue.</p> <p>Capturing metadata prior and during data collection is an aspect of step <code>4) data organization</code> in the soil information workflow, publishing the metadata as part of the data dissemination is an aspect of step <code>6) data and info sharing</code>. Evaluating the findability of datasets and assessing broken links on existing metadata is an aspect of step <code>7) Soil Information User Consideration</code>.</p> <p>In the soil domain we generally have 2 types of datasets, actual soil observations (calcium content in a horizon of a soil profile at a certain date or a soil profile field classification) and derived grids or polygon maps which represent parameter or soil type distribution for an area. For the second type of datasets describing the lineage (history) of the data is very important. Typically, you would describe the dataset with point observations in 1 document and link another document, describing the derived dataset, and link it as a parent-child relation. The document describing the derived dataset will contain 'processing-steps' describing the model that was used to calculate or derive the parameter or soil type distribution (D6.1 ch 5). This aspect is important for the usage of the derived dataset, to be able to evaluate if the estimate is valid for the envisioned use.</p> <p>If you missed the initial EJP Training on Soil data good practices, you can still have a look at the presentation about metadata and discovery.</p> <p>This page lists various implementation options both for creating metadata, as well as setting up a discovery service.</p>"},{"location":"metadata/#minimal","title":"Minimal","text":"<p>In a minimal implementation you can describe your dataset as well as your services in a single metadata document. This 'good practice' is described in https://github.com/INSPIRE-MIF/gp-data-service-linking-simplification. Basic metadata editors exist, of which the most basic is Notepad++. In the Python domain exists the pyGeoMeta and OWSLib projects, which offer capabilities to generate ISO19139 metadata from other formats.</p> <p>These metadata documents can be placed in a Web Accessible Folder. Products exist which are able to ingest documents from such a folder and expose it as a CSW discovery service. Such an ingest point could be installed at a national level, to facilitate the European INSPIRE GeoPortal (which currently only supports ingests via CSW).</p> Cookbook Software Description A pythonic metadata workflow pygeometa A minimalistic approach to data discovery Data in zenodo zenodo Zenodo is a data repository by CERN/Horizon2020, including rich metadata options"},{"location":"metadata/#traditional","title":"Traditional","text":"<p>The TG metadata (Technical Guidelines metadata) defines 2 types of metadata; documents which describe a dataset which are linked to documents which describe the service via which the datasets are published.</p> <p>The TG discovery describes how the metadata documents need to be published as a CSW discovery service. The table below lists some products which can be used to set up such a service. Mind that the TG extends the OGC CSW specification with some specific INSPIRE elements, for identification and multilingualism.</p> Cookbook Software Description GeoNetwork GeoNetwork A java based open source catalogue application widely used by member states for INSPIRE discovery. Provides a public portal application. Supports CSW, metadata authoring, validation and harvesting. pycsw pyCSW A python based open source CSW server. Supports CSW, OGC API Records. Used in portal software such as CKANSpatial and GeoNode. Geoportal server ArcGIS Geoportal A java based open source CSW implementation for the ArcGIS platform. A CSW client for ArcGIS desktop is included. Note that this package is not the same as ArcGIS Portal. Hale Connect Hale Connect A metadata authoring and CSW interface is provided as part of the HALE Connect SAAS offering."},{"location":"metadata/#experimental","title":"Experimental","text":"<p>A good practice exists related to Geo-DCAT-ap. It explains how to publish metadata using the Geo-DCAT-ap vocabulary as an additional metadata format. At present the use of ISO19139 is required by all guidelines. However, it is expected that it will soon be possible to offer metadata in a DCAT only. DCAT facilitates records to be discovered via google dataset search (and other search engines and semantic web platforms).</p> <p>Currently no 'good practice' exists to offer discovery services in alternative protocols then CSW. A good practice to adopt OGC API Records is being prepared. OpenSearch, OData and SPARQL could be alternative discovery service protocols.</p> Cookbook Software Description dcat - A dcat approach to dataset discovery"},{"location":"view/","title":"Overview INSPIRE View Services","text":"<p>The TG View services prescribes the adoption of view services, which offer the capability of visualization of spatial data, possibly in a portal, GIS software or webpage. The service provides a quick view on the data, without the need to transfer the data itself to the client. The TG Soil prescribes that for each measured soil parameter a view service <code>layer</code> is made available online. Layers can relate to actual site observations (soil profiles) as well as parameter distribution grids or vector maps.</p> <p>Setting up view services is an aspect of step <code>6) data and info sharing</code> in the soil information workflow.</p> <p>This page lists some implementation options for providing INSPIRE View Services.</p>"},{"location":"view/#minimal","title":"Minimal","text":"<p>In a minimal implementation the Web Map Tiling Service (WMTS) standard is used to provide view services. Tile services have little risk with respect to the Quality of Service requirements. The alternative option, Web Map Service (WMS), is quite prone to exceed the performance limits in cases when it has to 'draw' a lot of data at once.</p> <p>Tile services are however not optimal for dynamic data and may require a large (tile) storage. Also consider that the adoption of WMTS is less wide spread then WMS in GIS clients.</p> <p>The getFeatureInfo (gfi) operation is not mandatory for INSPIRE (however useful for end users). Without getFeatureInfo, data used as a source for the view service can be minimal (geometry only).</p> Cookbook Software Description mapserver Mapserver C based FastCGI WMS/WFS/WCS server implementation configured using 'mapfiles' Bridge &amp; GeoServer Bridge GeoServer The recipe describes how to publish view services from QGIS ArcMAP using GeoCat Bridge INSPIRE Mapproxy Python based tile (cache) server implementation, delegates on the fly rendering to a WMS server WMS from QGIS QGIS server The open source desktop GIS client deployed as a server application"},{"location":"view/#traditional","title":"Traditional","text":"<p>Most current view services are based on the Web Map Service (WMS) standard. These services are usually easy to set up on top of an existing traditional Web Feature Service or Web Coverage service implementation.</p> <p>Examples are in the download services section.</p>"},{"location":"view/#experimental","title":"Experimental","text":"<p>OGC API Tiles is an upcoming standard for map visualization. The Open Geospatial Consortium (OGC) is preparing the final standardization documents, however initial implementations are available in. There is no good practice document for adoption of OGC API Tiles within INSPIRE in preparation yet.</p> Cookbook Software Description GeoServer GeoServer OGC API Tiles is available via the OGC API community plugin Dive into pygeoapi pygeoapi Python package which exposes a cache of tiles as OGC API Tiles LDProxy LDProxy Java based opensource OGC API implementation <p>Within the sector there is a shift to the use of Vector tiles for vector map visualization. Vector tiles usually require less bandwidth and provide a sharper view on the data, especially on mobile devices with high resolution. INSPIRE does not provide Guidance on the use of vector tiles yet. The MapBox Vector tiles specification is a common API used to publish vector tiles.</p>"},{"location":"tools/52north/","title":"INSPIRE SOS download service using 52 North","text":"<p>Status: planned</p>"},{"location":"tools/52north/#read-more","title":"Read more","text":"<ul> <li>Webinar SOS and INSPIRE: https://www.youtube.com/watch?v=jyQJrTN4pjk</li> <li>Github: https://github.com/52North/SOS</li> <li>Docker hub: https://hub.docker.com/r/52north/sos</li> <li>OSGeo Live Quick Start: https://live.osgeo.org/en/quickstart/52nSOS_quickstart.html</li> <li>Specialised observations for INSPIRE: https://wiki.52north.org/SensorWeb/InspireSpecialisedObservations</li> </ul>"},{"location":"tools/bridge-geoserver-geonetwork/","title":"Metadata and View Service with GeoCat Bridge, GeoNetwork and GeoServer","text":"<p>Status: ready</p> <p>In this recipe we use GeoCat Bridge to publish a View Service on a soil dataset in GeoServer, combined with connected metadata in a GeoNetwork instance. </p> <p>GeoCat Bridge is a plugin for QGIS or ArcMap developed by GeoCat in Bennekom, the Netherlands. Its goal is to  facilitate the complex process of data publication from  a well known desktop environment. An introductionary video is available at https://www.youtube.com/watch?v=f-sZCVnR9dc</p> <p>GeoServer is a server application providing OGC services on various database backends. See the relevant recipe for a more detailed description. </p> <p>GeoNetwork is a server application providing a search interface and various api's on a collection of metadata records. See the relevant recipe for a more detailed description.</p>"},{"location":"tools/bridge-geoserver-geonetwork/#contents-of-the-recipe","title":"Contents of the recipe:","text":"<ul> <li>Deploy GeoServer and GeoNetwork using Docker</li> <li>Install and configure the Bridge plugin</li> <li>Publish the dataset</li> </ul>"},{"location":"tools/bridge-geoserver-geonetwork/#deploy-geoserver-and-geonetwork-using-docker","title":"Deploy GeoServer and GeoNetwork using Docker","text":"<p>This recipe is based on Docker, but you can also install each of the components directly on your system. New to Docker? Read the Docker recipe.</p> <p>We use the QGIS edition of Bridge, but an edition for ArcMAP is also available.</p> <p>Download the file https://github.com/ejpsoil/ejpsoil-data-publication-guidance/docker/bridge-geoserver-geonetwork/docker-compose.yml into an empty folder. Navigate to the folder using a shell client (windows powershell or Linux/Apple shell) and run:</p> <pre><code>docker compose up\n</code></pre> <p>The above statement will download and deploy docker containers for GeoServer, GeoNetwork, PostGreSQL and Elastic Search. When finished (it may take a long time), you can access GeoServer at https://localhost:8000/geoserver and GeoNetwork at https://localhost:8001/geonetwork.</p> <p>GeoNetwork starts with an error, because the database is empty. Navigate to <code>http://localhost:8001/geonetwork/srv/eng/admin.console#/metadata</code>, login as user: <code>admin</code>, password: <code>admin</code>. Select the <code>iso19139:2007</code> profile and click <code>Load templates</code> and <code>Load samples</code>.</p>"},{"location":"tools/bridge-geoserver-geonetwork/#install-and-configure-the-bridge-plugin","title":"Install and configure the Bridge plugin","text":"<p>With QGIS, open the QGIS plugins repository and search for the <code>GeoCat Bridge</code> plugin. Install it.</p> <p>After succesfull installation, we will configure our GeoServer and GeoNetwork instances.</p> <p>Find the Bridge module on the toolbar or in the <code>Web &gt; GeoCat Bridge &gt; Publish</code> menu. From the publish window, open the <code>Servers</code> tab.</p> <p>In the bottom left click the <code>New server</code> option and select <code>GeoServer</code>. Populate the fields.</p> <p>Click <code>New server</code> again and select <code>GeoNetwork</code>. Populate the fields.</p> <p>You are now ready to publish your first dataset from QGIS.</p>"},{"location":"tools/bridge-geoserver-geonetwork/#publish-the-dataset","title":"Publish the dataset","text":"<p>Open the dataset to be published. Configure the layer with relevant styling and labels.</p> <p>Open the publish window from the menu (or toolbar). </p> <p>Select the layer to be published. A metadata editor will open in which you can configure some metadata. You can also import embedded metadata (use the button with a downward arrow, top right).</p> <p>Select the target servers for the publisation and click the publish button.</p> <p>When returning to the publish panel, you will notice an icon behind each layer indicating the publication status. You can now right click on the layer to preview the wms layer or the metadata.</p> <p>| Note that the styling options in QGIS and GeoServer are not a full match. Some styling transformations are applied which may impact the final result on GeoServer. A common caveat is the availability of certain fonts, used for labeling or icons, on the client and the server. Make sure all used fonts are available on the server as well. |</p>"},{"location":"tools/bridge-geoserver-geonetwork/#validate-the-view-service","title":"Validate the view service","text":"<p>The INSPIRE Validator provides a validation of view services. It will mainly test if metadata elements are available and the service is reachable.</p> <p>The docker containers run locally, so the services can not be tested by the INSPIRE Validator.  In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.</p> <p>We have not yet installed the INSPIRE plugin on GeoServer and optimized the configuration of GeoNetwork, so expect some tests to fail</p>"},{"location":"tools/codelist-iso19135/","title":"Publish a codelist as a file in a web accessible folder","text":"<p>status: draft</p> <p>This recipe presents a minimal approach for publishing a (extended) code list. </p> <p>The most rich and common format for code list storage is RDF (turtle, json-ld, or rdf/xml) using the SKOS ontology. It is the format used by for example Glosis to publish its codelists. Tools like GeoNetwork are able to ingest codelists based on the SKOS ontology.</p> <p>An alternative format is based on the iso19135:2005 standard. TC 211 develops the iso19135 standard to offer a format for code lists in the spatial data community. The standard is for example used in the gmx-codelists as used in iso19139:2007.</p> <p>The minimal approach described here is for example used in Lituania. From the SoilBody GML the dataset links to locally extended codelists published in an XML format inspired by the INSPIRE registry. See https://inspire-geoportal.lt/resources/codelist/SO/OtherHorizonNotationTypeValue.xml. This format is probably selected because it is supported by Hale Studio.</p> <p>The location of the codelist can be a webserver folder, a git repository, even a shared sharepoint folder. But it is important that the url of the file is persistent for at least 10+ years. Because the datasets which link to codelist items depend on its availability. A single file typically lists a number of concepts, or even concept schemes. You can reference a concept within the file by concatenating the concept identifier to the url with \"#\", e.g. https://inspire-geoportal.lt/resources/codelist/SO/OtherHorizonNotationTypeValue.xml#Ap). </p> <p>Persistency can be improved by adding an intermediary layer between the storage of the file and the url on which it is made available, mechanisms such as provided by DOI and W3ID.Glosis codelists are for example stored at https://github.com/rapw3k/glosis but are referenced as http://w3id.org/glosis/model/codelists and then forwarded.</p>"},{"location":"tools/codelist-iso19135/#extending-inspire-codelists","title":"Extending INSPIRE codelists","text":"<p>In this article Wetransform explains how and which INSPIRE codelists can be extended. Either the UML model and/or the INSPIRE registry indicate if a codelist is extensible.</p> <p>A common approach for extending codelists is to create a new codelist file and duplicate similar terms from the common codelist and link back to them with a <code>SameAs</code> link. Broader or Narrower concepts can be added with a broader or narrower link. </p>"},{"location":"tools/codelist-iso19135/#content-negotiation","title":"Content negotiation","text":"<p>When a human arrives at a codelist file, the syntax with http://example.com#concept opens the full file, and does not point to the relavant concept, because the web browser does not understand the file format. The mechanism of content negotiation can identify web browsers and present them an alternative format (html). Content negotiation can be set up in an intermediary webserver layer, the transformation from SKOS/iso19135 to html should be managed by an extra utility (such as SKOSMOS).</p>"},{"location":"tools/codelist-iso19135/#multilingual-concept-labels","title":"Multilingual concept labels","text":"<p>Both SKOS and ISO19135 support the option to provide labels for concepts in multiple languages.</p>"},{"location":"tools/deegree/","title":"deegree","text":"<p>Status: contribution required</p> <p>An open source java server implementation of WMS, WMTS, CSW, WCS, WFS, WPS.</p> <p>Start a deegree instance locally using the docker hub image as:</p> <pre><code>docker run --name deegree -d -p 8080:8080 deegree/deegree3-docker\n</code></pre> <p>Initial start will take some time, then proceed with your browser to http://localhost:8080/deegree-webservices.</p> <p>The deegree website contains a detailed quick start on how to import and operate a sample workspace</p> <p>Setting up a database and importing GML Data is managed via a command line client. The command line client can be accessed via:</p> <pre><code>docker exec -w /opt deegree java -jar deegree-tools-gml.jar -help\n</code></pre> <p>The client tools are described in the online manual. An example call to export a database creation script to reflect the Soil.xsd schema:</p> <pre><code>docker exec -w /opt/ deegree java -jar deegree-tools-gml.jar SqlFeatureStoreConfigCreator --format=ddl --dialect=postgis --cycledepth=1 -schemaUrl=https://inspire.ec.europa.eu/schemas/so/4.0/Soil.xsd\n</code></pre>"},{"location":"tools/deegree/#read-more","title":"Read more","text":"<p>deegree is maintained by a company called LatLon and others</p> <ul> <li>Website: http://www.deegree.org/</li> <li>Documentation: https://download.deegree.org/documentation/3.4.32/html/</li> <li>Download: https://www.deegree.org/download/</li> <li>Docker: https://hub.docker.com/r/deegree/deegree3-docker/</li> </ul>"},{"location":"tools/fme/","title":"FME","text":"<p>Status: contribution required</p> <p>Safe software propides a proprietary solution for data harmonization, e.g. conversion to and from INSPIRE data models. An introduction to the topic is provided at https://www.safe.com/integrate/inspire-gml/</p>"},{"location":"tools/frost-server/","title":"FROST server","text":"<p>Status: contribution required</p> <p>Frost is an open source server implementation of OGC Sensorthings, a modern data exchange standard for Sensor Data.  Because the INSPIRE Soil model is based on Observations and Measurements, the Sensorthings API can be used  to provide Soil related data download services. Sensorthings API is generally easier to set up for administrators and easier to consume by  clients then INSPIRE Soil data in GML.</p> <ul> <li>Github repository: https://github.com/FraunhoferIOSB/FROST-Server</li> <li>Documentation: https://fraunhoferiosb.github.io/FROST-Server/</li> <li>Workshop: https://datacoveeu.github.io/API4INSPIRE/dissemination/Workshop-2020-11-19.html</li> </ul>"},{"location":"tools/geohealthcheck/","title":"GeoHealthCheck","text":"<p>Status: ready</p> <p>A tool to monitor availability of spatial services. The tool will query a list of configured spatial services at intervals and report on availability using charts. The tool can also send out notifications in case of disruptions.</p> <p>The tool can be compared to (and is often combined with) generic web availability tools such as Zabbix, Uptimerobot, Nagios. The generic tools are used to identify if the service is up, GeoHealthCheck will go a level deeper, identify which layers are available in a getcapabilities response and ask random maps to individual layers to identify if a service is properly running.</p>"},{"location":"tools/geohealthcheck/#exercise","title":"Exercise","text":"<p>The recipe assunes docker desktop installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click <code>register</code> in the login page). Start a local GeoHealthCheck container:</p> <pre><code>docker run --name ghc -p80:80 geopython/geohealthcheck\n</code></pre> <ul> <li>Visit http://localhost</li> <li>Login as user: <code>admin</code>, password: <code>admin</code></li> <li>Click <code>ADD +</code> on the top bar right, select <code>WMS</code></li> <li>Add a WMS url, for example <code>https://maps.isric.org/mapserv?map=/map/wrb.map</code></li> <li>On the next screen click <code>add</code> for <code>WMS Drilldown</code> (so all layers are validated)</li> <li>Click <code>Save</code> and <code>test</code></li> <li>When finished, click <code>Details</code> to see the test result</li> </ul>"},{"location":"tools/geonetwork/","title":"A discovery service in GeoNetwork","text":"<p>Status: in progress</p> <p>GeoNetwork is a catalogue for registering spatial datasets and services. GeoNetwork does support multiple metadata models based on XML, but it is optimized for iso19139:2007 and iso19115-2:2018. This recipe uses docker to run GeoNetwork locally. It will discuss aspects such as schema plugins, creating metadata records, set up codelists and harvest metadata.</p>"},{"location":"tools/geonetwork/#schema-plugins","title":"Schema plugins","text":"<p>GeoNetwork provides a dynamic system to pre load schema plugins providing support for a variety of metadata models, such as iso19139:2007, iso19115-2:2018, national profiles based on these, DCAT, SensorML, EML/GBIF, etc. Many of these plugins are made available via https://github.com/metadata101. Before creating or importing records, verify that the relevant profile is available in the GeoNetwork instance.</p> <p>Creation of metadata records is based on templates. For each metadata model a series of templates is available. Users select a relevant template for their use case while creating a new record to describe a resource. Any template is based on a specific metadata model, which determines which properties need to be described and in which format the record will be stored. </p> <p>GeoNetwork provides a number of transformation options for crosswalks between metadata models, but you should always consider some loss of information in these crosswalks. Crosswalks occur for example when a user requests a record in DCAT, while it is stored as iso19139:2007 in the database.</p>"},{"location":"tools/geonetwork/#quick-start","title":"Quick Start","text":"<ul> <li>Start a GeoNetwork instance locally (initial startup may take some time)</li> </ul> <p><code>docker run -p8080:8080 geonetwork:3.12</code></p> <ul> <li>Navigate to http://localhost:8080/geonetwork</li> <li>Let's load some sample data. Select <code>Login</code>; login as usr:<code>admin</code>, pwd:<code>admin</code></li> <li>Select <code>Admin console</code> &gt; <code>Metadata records and templates</code>; Select <code>iso19139:2007</code>, click on <code>load samples</code> and <code>load templates</code></li> <li>On <code>Admin console</code> &gt; <code>Settings</code>, set the title and url of the instance.</li> <li>Let's set up some code lists (which populate the pull downs on the editor). Select <code>Classification systems</code> from <code>Admin console</code>.</li> <li>Select <code>From registry</code> in <code>Add thesaurus</code>.</li> <li>Click <code>Use INSPIRE registry</code> for the <code>Url</code> field</li> <li>Select language(s) and <code>INSPIRE theme register</code> and click <code>Upload</code></li> <li>Add another thesaurus from registry, select <code>INSPIRE metadata code list register</code> and then <code>Spatial scope</code>.</li> <li>Continue with other relevant code lists, notice that you can also create a new code list manually.</li> </ul>"},{"location":"tools/geonetwork/#create-records","title":"Create records","text":"<ul> <li>On <code>Contribute</code> Editor board, click <code>Add new record</code></li> <li>Select a template (they we're loaded on the previous step) and click <code>create</code></li> <li>On the editor notice the view (eye) button on top right, you can switch the editor view between <code>Simple</code>, <code>Full</code> and <code>XML</code>.</li> <li>Notice the <code>validate</code> button, which provides a report on the level of completion of the record</li> <li>The associated resources side panel displays links to remote resources, such as data files, data services and thumbnails</li> <li>Notice that you can also import a record from a local xml file</li> <li>Notice that you can collapse the <code>save</code> button to <code>save as template</code>, in this way others can use this record as a base to start a new record</li> </ul>"},{"location":"tools/geonetwork/#harvest-records","title":"Harvest records","text":"<p>Harvesting is the process of importing records from remote sources at intervals.</p> <ul> <li>Open <code>harvesting</code> from <code>Admin console</code></li> <li>Select <code>OGC CSW 2.0.2</code> from <code>Add thesaurus</code></li> <li>Provide a name for the harvester</li> <li><code>Action on UUID collision</code> determines the behaviour when similar records are found in multiple remote endpoints</li> <li>Provide the url of the remote endpoint (for example https://www.geocatalogue.fr/api-public/servicesRest?request=GetCapabilities&amp;service=CSW)</li> <li>Add a filter <code>Anytext:Soil</code></li> <li>Set a interval schedule for the harvest (<code>only one run</code>)</li> <li>Harvesters have many additional options, you can read about it in the documentation</li> <li>Click <code>Save</code> and on the next screen <code>harvest</code>, a spinner starts to run, the harvest may take some minutes, depending of the size of the remote catalogue</li> <li>Notice a list of harvest run logs at the bottom of the screen, you can click <code>log</code> to check in more detail.</li> <li>If you increase logging on <code>Settings</code> to <code>DEV</code>, the logging on <code>harvesting</code> will also provide more details (in case of non explainable errors)</li> </ul>"},{"location":"tools/geonetwork/#discovery-from-qgis","title":"Discovery from QGIS","text":"<p>The <code>metasearch</code> plugin is a default plugin in QGIS.  - Open the plugin from the <code>Web</code> menu (or toolbar).  - Click <code>New connection</code>. Provide a name for the connection and the url http://localhost:8080/geonetwork/srv/eng/csw - Switch to the find tab, and search some records.  - Select a search result, for some search results the <code>load data</code> button (lower left) is activated and you can load some data to the map</p>"},{"location":"tools/geonetwork/#atom-download-service-from-geonetwork","title":"Atom download service from GeoNetwork","text":"<p>You can enable an ATOM download service in GeoNetwork. GeoNetwork provides an opensearch API and will use the metadata content to generate Atom service and dataset files. You can read more about this option in the documentation.</p>"},{"location":"tools/geonetwork/#read-more","title":"Read more","text":"<ul> <li>Website: https://geonetwork-opensource.org/</li> <li>Github repository: https://github.com/geonetwork</li> <li>Docker composition: https://github.com/geonetwork/docker-geonetwork/blob/main/4.2.1/docker-compose.yml</li> <li>Documentation: https://geonetwork-opensource.org/manuals/4.0.x/en</li> <li>Tutorial: https://geonetwork-opensource.org/manuals/trunk/en/tutorials/introduction</li> </ul>"},{"location":"tools/geoportal-server/","title":"Esri Geoportal server","text":"<p>Status: contribution required</p> <p>Esri Geoportal Server provides seamless communication with data services that use a wide range of communication protocols, and also supports searching, publishing, and managing standards-based resources.</p> <p>Website: https://enterprise.arcgis.com/en/inspire/10.8/get-started/introduction-to-geoportals.htm Github: https://github.com/Esri/geoportal-server-catalog/blob/master/docker/gpt_stack/geoportal/Dockerfile</p>"},{"location":"tools/geoserver/","title":"Appschema WFS in GeoServer","text":"<p>Status: contribution required</p> <p>GeoServer is an open source java imlementation of WFS, WCS, WPS, WMS, CSW. Various OGC API endpoints are available via  a OGCAPI community plugin. WMTS is available via a default plugin called <code>GeoWebCache</code>.</p> <p>GeoServer is a popular server component because of the initial ease of setup and configuration in a webbased environment. It includes an authentication and authorisation system and advanced styling options. Configuration via a webinterface also has some negative aspects related to reproducability and scaling. GeoServer is able to provide INSPIRE data via the appschema plugin and (INSPIRE plugin)[https://docs.geoserver.org/stable/en/user/extensions/inspire].</p>"},{"location":"tools/geoserver/#run-as-docker-container","title":"Run as docker container","text":"<pre><code>docker run -p8080:8080 kartoza/geoserver:2.22.0\n</code></pre> <ul> <li>Navigate to http://localhost:8080/geoserver</li> <li>Login as usr:admin pwd:geoserver</li> </ul> <p>Load some data:</p> <ul> <li>create a workspace</li> <li>create a datastore of type <code>folder of shapefiles</code></li> <li>create a layer</li> </ul> <p>Preview the layer.</p> <p>Consider that in this setup the configuration is lost at every restart of the container. In a normal scenario, you would mount a volume to persist the geoserver configuration. Optimally you place the volume under version control, so you can easily revert a previous situation.</p>"},{"location":"tools/geoserver/#inspire-plugin","title":"INSPIRE plugin","text":"<p>A GeoServer INSPIRE plugin is available which adds some of the INSPIRE specific metadata properties to the OWS capabilities documents. For example a link to the service metadata. The main feature is that it adds a bbox for each of the available projection systems. GeoServer is known to list all projection systems (many) as part of the capabilities response. You need to limit this number to prevent the bounds be written in each of this projections.</p>"},{"location":"tools/geoserver/#appschema-support","title":"Appschema support","text":"<p>Appschema is a plugin for GeoServer which adds the capability to work with hierarchival GML data, such as the INSPIRE Soil data model.</p> <p>Onegeology has prepared a workshop on how to set up an appschema dataset in GeoServer. This is an advanced workshop. </p> <p>At Foss4G 2022 in Florence one of the maintainers of GeoServer, GeoSolutions, announced a new approach to appschema in GeoServer, based on templating. I have not been able to test it yet, but it may resolve some of the challenges of the appschema approach.</p>"},{"location":"tools/geoserver/#geoserver-as-a-view-service","title":"GeoServer as a View service","text":"<p>GeoServer also provides options to publish view services (WMS or WMTS). Read more about this topic in the recipe GeoCat Bridge and GeoServer.</p>"},{"location":"tools/geoserver/#read-more","title":"Read more","text":"<p>Website: https://geoserver.org Github: https://github.com/geoserver/ Docker: https://docker.osgeo.org/geoserver Issue management: https://osgeo-org.atlassian.net/projects/GEOS/summary OSGeo: https://www.osgeo.org/projects/geoserver/</p>"},{"location":"tools/glosis-db/","title":"Glosis as GeoPackage","text":"<p>This recipe desbribes an approach where data is harmonised to a glosis oriented relational database.</p>"},{"location":"tools/hale-connect/","title":"Hale Connect","text":"<p>Status: contribution required</p> <p>Hale Connect is a Software as a Service solution provided by wetransform to provide view and download services with metadata for rich datasets, such as those following the INSPIRE data models. Transformations are prepared in Hale Studio and effectuated in the Hale Connect Solution.</p> <p>The services are  optimised for ease of use and performance, due to the cloud native setup of the data services. A data space connector is in preparation.</p> <ul> <li>Website: https://wetransform.to/haleconnect</li> <li>Get started at https://help.wetransform.to/docs/getting-started/2018-04-28-quick-start </li> </ul> <p>You can benefit from a 14-day free trial period when signing up for the platform.</p>"},{"location":"tools/hale-studio-consume-gml/","title":"Consume Soil GML with Hale Studio","text":"<p>Status: in progress</p> <p>Hale Studio is a familiar tool to transform data from a relational datamodel to INSPIRE Soil GML. However you can also use Hale Studio to connect to an existing WFS or load a GML file from a atom service and use Hale Studio to transform the data to a relational database model, so you can easily combine it with other relational databases.</p> <p>Note</p> <p>If you're not interested in a specific target model, the GMLAS functionality within OGR may be sufficient for you. GMLAS will create a arbitrary relational model from any GML. With GDAL installed, and a gml file called soil.gml, run the following from commandline:</p> <p>ogrinfo -ro GMLAS:soil.gml</p> <p>ogr2ogr -f SQLite tmp.sqlite GMLAS:soil.gml -dsco SPATILIATE=YES -nlt CONVERT_TO_LINEAR -oo EXPOSE_METADATA_LAYERS=YES</p> <p>In this recipe we will transform INSPIRE Soil GML from the city of Berlin to a relational database (GeoPackage).</p> <ul> <li>(Install and) Start the Hale Studio tool</li> <li>Import soil data from the Berlin Soil at https://fbinter.stadt-berlin.de/fb/atom/SO/SO_KrBwBoF2015.zip</li> <li>unzip the file to a new folder</li> <li>In hale studio, at File &gt; Import &gt; Source model, select the soil.xml included in the zip file</li> <li>At File &gt; Import &gt; Source data, select the <code>INSPIRE GML...gml</code> included in the zip file, select default options on the import wizard</li> <li>At File ...</li> </ul>"},{"location":"tools/hale-studio/","title":"HALE Studio","text":"<p>Status: in progress</p> <p>HALE Studio aims to enable users to set up a harmonization workflow on datasets from a local model to a common model, such as INSPIRE. The user interface presents the model of the source dataset (derived from database) on the left and the target model on the right (derived from xml schema). Conversion rules are defined by selecting similar properties on both sides.</p> <p>The open source software has been developed in the scope of a European Research project, HUMBOLDT (2006) and is currently maintained by a company called WeTransform in Darmstad Germany. WeTransform hosts the Hale Studio user guide and a user forum. The Git repository for Hale Studio is at https://github.com/halestudio/hale.</p> <p>This recipe has been developed in the scope of a Masterclass on data assimilation within the EJP Soil project and builds on harmonization work performed in the scope of the eDanube project.</p> <p>In this recipe we'll harmonize a SOTER database to the INSPIRE model. Read more about the INSPIRE Soil model in the relevant technical guidelines. </p> <p>Contents of the recipe</p> <ul> <li>SOTER database</li> <li>Proparing the data </li> <li>Install &amp; get started with Hale Studio</li> <li>Define harmonization rules</li> <li>Export GML</li> </ul>"},{"location":"tools/hale-studio/#soter-database","title":"SOTER Database","text":"<p>For this recipe we're going to use the SOTER database of Cuba. Download the zip file from https://data.isric.org/geonetwork/srv/eng/catalog.search#/metadata/f31ac19f-67a4-4f64-94cc-d4f063ea9add. </p> <p>The SOTER programme was initiated in 1986 by the Food and Agricultural Organization of the United Nations (FAO), the United Nations Environmental Programme and ISRIC, under the auspices of the International Soil Science Society. The aim of the programme was to develop a global SOTER database at scale 1:1 million that was supposed to be the successor of the FAO-UNESCO Soil Map of the World. A SOTER database with global coverage was never achieved, but SOTER databases were developed for various regions, countries and continents.</p> <p>The picture below shows the database structure of a SOTER database. The database structure allows to store terrain, soil profile up to wet chemistry results.</p> <p></p> <p>In this recipe we're focussing on the <code>RepresentativeHorizonValues</code> table mostly, which contains observed properties for each horizon. </p>"},{"location":"tools/hale-studio/#preparing-the-data","title":"Preparing the data","text":"<p>Notice that, like many other soil databases, the observed soil property values are listed as columns for each horizon. The INSPIRE model instead uses the Observations and Measurments model, in which each observation is an individual entity which includes the feature of interest (e.g. <code>Horizon 2</code>), the observed property (e.g. <code>pH</code>), the result (e.g. <code>7.2</code>) and the process (e.g. <code>pHCaCl2</code>). </p> <p></p> <p>A data transformation required for this step is challenging within Hale, but relatively easy within the database. So before starting up Hale we'll make an initial transformation within the database.</p> <p>The SOTER zip file contains a SQLite as well as a Access version of the database. In this recipe we'll work with Access, but you can also use the SQLite version, in that case install for example SQLite browser to interact with the database. Some of the queries may slightly vary between SQLite and Access.</p> <p>Run the query below, to create a new <code>OBSERVATIONS</code> table. </p> <p>In Access create a new <code>query</code> in <code>design view</code>. </p> <p></p> <p>Then open <code>SQL view</code>.</p> <p></p> <p>Run the query, by clicking <code>Run</code> in the <code>Query design</code> toolbar.</p> <pre><code>SELECT * INTO OBSERVATIONS\nFROM (Select HONU,PRID,'SCMO' as PARAM,SCMO as RESULT from RepresentativeHorizonValues where SCMO is not null union\nSelect HONU,PRID,'SCDR' as PARAM,SCDR as RESULT from RepresentativeHorizonValues where SCDR is not null union\nSelect HONU,PRID,'STGR' as PARAM,STGR as RESULT from RepresentativeHorizonValues where STGR is not null union\nSelect HONU,PRID,'STSI' as PARAM,STSI as RESULT from RepresentativeHorizonValues where STSI is not null union\nSelect HONU,PRID,'STTY' as PARAM,STTY as RESULT from RepresentativeHorizonValues where STTY is not null union\nSelect HONU,PRID,'SDVC' as PARAM,SDVC as RESULT from RepresentativeHorizonValues where SDVC is not null union\nSelect HONU,PRID,'SDCO' as PARAM,SDCO as RESULT from RepresentativeHorizonValues where SDCO is not null union\nSelect HONU,PRID,'SDME' as PARAM,SDME as RESULT from RepresentativeHorizonValues where SDME is not null union\nSelect HONU,PRID,'SDFI' as PARAM,SDFI as RESULT from RepresentativeHorizonValues where SDFI is not null union\nSelect HONU,PRID,'SDVF' as PARAM,SDVF as RESULT from RepresentativeHorizonValues where SDVF is not null union\nSelect HONU,PRID,'SDTO' as PARAM,SDTO as RESULT from RepresentativeHorizonValues where SDTO is not null union\nSelect HONU,PRID,'STPC' as PARAM,STPC as RESULT from RepresentativeHorizonValues where STPC is not null union\nSelect HONU,PRID,'CLPC' as PARAM,CLPC as RESULT from RepresentativeHorizonValues where CLPC is not null union\nSelect HONU,PRID,'PSCL' as PARAM,PSCL as RESULT from RepresentativeHorizonValues where PSCL is not null union\nSelect HONU,PRID,'BULK' as PARAM,BULK as RESULT from RepresentativeHorizonValues where BULK is not null union\nSelect HONU,PRID,'ELCO' as PARAM,ELCO as RESULT from RepresentativeHorizonValues where ELCO is not null union\nSelect HONU,PRID,'SSO4' as PARAM,SSO4 as RESULT from RepresentativeHorizonValues where SSO4 is not null union\nSelect HONU,PRID,'HCO3' as PARAM,HCO3 as RESULT from RepresentativeHorizonValues where HCO3 is not null union\nSelect HONU,PRID,'SCO3' as PARAM,SCO3 as RESULT from RepresentativeHorizonValues where SCO3 is not null union\nSelect HONU,PRID,'EXCA' as PARAM,EXCA as RESULT from RepresentativeHorizonValues where EXCA is not null union\nSelect HONU,PRID,'EXMG' as PARAM,EXMG as RESULT from RepresentativeHorizonValues where EXMG is not null union\nSelect HONU,PRID,'EXNA' as PARAM,EXNA as RESULT from RepresentativeHorizonValues where EXNA is not null union\nSelect HONU,PRID,'EXCK' as PARAM,EXCK as RESULT from RepresentativeHorizonValues where EXCK is not null union\nSelect HONU,PRID,'EXAL' as PARAM,EXAL as RESULT from RepresentativeHorizonValues where EXAL is not null union\nSelect HONU,PRID,'EXAC' as PARAM,EXAC as RESULT from RepresentativeHorizonValues where EXAC is not null union\nSelect HONU,PRID,'CECS' as PARAM,CECS as RESULT from RepresentativeHorizonValues where CECS is not null union\nSelect HONU,PRID,'TCEQ' as PARAM,TCEQ as RESULT from RepresentativeHorizonValues where TCEQ is not null union\nSelect HONU,PRID,'GYPS' as PARAM,GYPS as RESULT from RepresentativeHorizonValues where GYPS is not null union\nSelect HONU,PRID,'P2O5' as PARAM,P2O5 as RESULT from RepresentativeHorizonValues where P2O5 is not null union\nSelect HONU,PRID,'PRET' as PARAM,PRET as RESULT from RepresentativeHorizonValues where PRET is not null union\nSelect HONU,PRID,'FEDE' as PARAM,FEDE as RESULT from RepresentativeHorizonValues where FEDE is not null union\nSelect HONU,PRID,'PHAQ' as PARAM,PHAQ as RESULT from RepresentativeHorizonValues where PHAQ is not null union\nSelect HONU,PRID,'PHKC' as PARAM,PHKC as RESULT from RepresentativeHorizonValues where PHKC is not null union\nSelect HONU,PRID,'SONA' as PARAM,SONA as RESULT from RepresentativeHorizonValues where SONA is not null union\nSelect HONU,PRID,'SOCA' as PARAM,SOCA as RESULT from RepresentativeHorizonValues where SOCA is not null union\nSelect HONU,PRID,'SOMG' as PARAM,SOMG as RESULT from RepresentativeHorizonValues where SOMG is not null union\nSelect HONU,PRID,'SOLK' as PARAM,SOLK as RESULT from RepresentativeHorizonValues where SOLK is not null union\nSelect HONU,PRID,'SOCL' as PARAM,SOCL as RESULT from RepresentativeHorizonValues where SOCL is not null union\nSelect HONU,PRID,'FEPE' as PARAM,FEPE as RESULT from RepresentativeHorizonValues where FEPE is not null union\nSelect HONU,PRID,'ALDE' as PARAM,ALDE as RESULT from RepresentativeHorizonValues where ALDE is not null union\nSelect HONU,PRID,'CLAY' as PARAM,CLAY as RESULT from RepresentativeHorizonValues where CLAY is not null union\nSelect HONU,PRID,'TOTC' as PARAM,TOTC as RESULT from RepresentativeHorizonValues where TOTC is not null union\nSelect HONU,PRID,'TOTN' as PARAM,TOTN as RESULT from RepresentativeHorizonValues where TOTN is not null  \n);\n</code></pre> <p>Verify that a new table <code>OBSERVATIONS</code> is available and that it is populated.</p>"},{"location":"tools/hale-studio/#install-hale-studio","title":"Install Hale Studio","text":"<p>Download and install Hale Studio from github. There are installers for windows, linux and apple.  Kate Lyndegaard from WeTransform published a nice overview of Hale Studio at https://www.youtube.com/watch?v=BKNMV-Jp9HM&amp;t=332s.</p> <ul> <li>First create a new project </li> <li>Import the SOTER database as <code>source schema</code>. </li> <li>Import the same database file again as 'source data'. </li> <li>Repeat these 2 steps for the Cuba shapefile, available in the GIS folder of the zip file </li> <li>Load the INSPIRE Soil model as a target schema. Load the latest version of the model from https://inspire.ec.europa.eu/schemas/so/4.0/Soil.xsd (<code>from url</code> tab, click   <code>detect</code> after entering the url).</li> </ul> <p></p>"},{"location":"tools/hale-studio/#define-harmonization-rules","title":"Define harmonization rules","text":"<p>We'll go through some cases to highlight some of the features, we'll not produce a full mapping.</p>"},{"location":"tools/hale-studio/#join-tables","title":"Join tables","text":"<p>In order to link the geometries from the shapefile to the SOTER data, we'll use a table join. On the left column, select the shapefile as well as the <code>terrain</code>, <code>soils</code> and <code>soilscomponent</code> tables (ctrl-click). On the right colum select the SoilBody type. Now click the blue arrow in the middle and select the <code>join</code> method.</p>"},{"location":"tools/hale-studio/#link-or-embed-and-identification","title":"Link or embed and identification?","text":"<p>XML allows to embed a property or to reference the value of the property elsewhere. An example; both snippets below have the same meaning, the first is easier to read, the second is easier to handle by software (prevent duplication).</p> <pre><code>&lt;person role=\"student\" name=\"Peter\"&gt;\n    &lt;memberOf&gt;\n        &lt;class name=\"2B\"&gt;\n            &lt;hasMember&gt;\n                &lt;person role=\"teacher\" name=\"Cynthia\"&gt;\n            &lt;/hasMember&gt;\n        &lt;/class&gt;\n    &lt;memberOf&gt;\n&lt;/person&gt;\n</code></pre> <p>And</p> <pre><code>&lt;person role=\"student\" name=\"Peter\" gml:id=\"#student-peter\"&gt;\n    &lt;memberOf xlink:href=\"#class-2b\"&gt;\n&lt;/person&gt;\n&lt;person role=\"teacher\" name=\"Cynthia\" gml:id=\"#teacher-cynthia\"&gt;\n    &lt;memberOf xlink:href=\"#class-2b\"&gt;\n&lt;/person&gt;\n&lt;class name=\"2B\" gml:id=\"#class-2b\"/&gt;\n</code></pre> <p>A good practice is to add reverse links to the second snippet:</p> <pre><code>&lt;class name=\"2B\" gml:id=\"#class-2b\"&gt;\n    &lt;hasMember xlink:href=\"#teacher-cynthia\"/ &gt;\n    &lt;hasMember xlink:href=\"#student-peter\" /&gt;\n&lt;/class&gt;\n</code></pre> <p>Both approaches are supported and can be combined in Hale Studio, but you have to consider upfront which approach to use when. The first approach becomes quite complex if the levels of nesting increase.</p> <p>A suggestion from our side; define Plot, Profile, OM_Observation and Laboratory as root types and embed other types.</p>"},{"location":"tools/hale-studio/#anytype-in-xsd","title":"Anytype in XSD","text":"<p>XSD allows to leave the type of a property as <code>any</code>. From a standardisation perspective, this is not optimal, because every developer may implement a different type for that field. In the INSPRE Soil theme this challenge is very obvious because the type of the result property of an observation is defined as <code>any</code>. Hale Studio is not able to process <code>anytype</code> fields by default. Instead you have to add below snippet to the <code>eu.esdihumboldt.hale.io.schema.read.target</code> resource of the  <code>project.halex</code> file. </p> <pre><code> &lt;complex-setting name=\"customTypeContent\"&gt;\n    &lt;xsd:typeContentConfig xmlns:xsd=\"http://www.esdi-humboldt.eu/hale/io/xsd\"&gt;\n        &lt;core:list xmlns:core=\"http://www.esdi-humboldt.eu/hale/core\"&gt;\n            &lt;core:entry&gt;\n                &lt;xsd:association&gt;\n                    &lt;xsd:property&gt;\n                        &lt;core:list&gt;\n                            &lt;core:entry&gt;\n                                &lt;core:name namespace=\"http://www.opengis.net/om/2.0\"&gt;OM_ObservationType&lt;/core:name&gt;\n                            &lt;/core:entry&gt;\n                            &lt;core:entry&gt;\n                                &lt;core:name namespace=\"http://www.opengis.net/om/2.0\"&gt;result&lt;/core:name&gt;\n                            &lt;/core:entry&gt;\n                        &lt;/core:list&gt;\n                    &lt;/xsd:property&gt;\n                    &lt;xsd:config&gt;\n                        &lt;xsd:typeContent mode=\"elements\"&gt;\n                            &lt;xsd:elements&gt;\n                                &lt;core:list&gt;\n                                    &lt;core:entry&gt;\n                                        &lt;core:name namespace=\"http://www.isotc211.org/2005/gco\"&gt;CharacterString&lt;/core:name&gt;\n                                    &lt;/core:entry&gt;\n                                &lt;/core:list&gt;\n                            &lt;/xsd:elements&gt;\n                        &lt;/xsd:typeContent&gt;\n                    &lt;/xsd:config&gt;\n                &lt;/xsd:association&gt;\n            &lt;/core:entry&gt;\n        &lt;/core:list&gt;\n    &lt;/xsd:typeContentConfig&gt;\n&lt;/complex-setting&gt;\n</code></pre> <p></p>"},{"location":"tools/hale-studio/#export-gml","title":"Export GML","text":""},{"location":"tools/inspire-geoportal/","title":"INSPIRE Geoportal","text":"<p>Status: in progress</p> <p>The INSPIRE Geoportal is the central European access point to the data provided by EU Member States and several EFTA countries under the INSPIRE Directive.</p> <ul> <li>Website: https://inspire-geoportal.ec.europa.eu/</li> <li>Github: https://github.com/INSPIRE-MIF/helpdesk-geoportal</li> </ul>"},{"location":"tools/inspire-geoportal/#harvesting-metadata","title":"Harvesting metadata","text":"<p><code>Harvest</code> is the process of copying metadata from a remote source (catalogue) to the european GeoPortal. </p> <p>For each memberstate a contact point is assigned, which is responsible for registering the national endpoint(s) to be harvested by the geoportal. The national contact point also triggers the harvest and validates the result before publishing it. At the harvest status page. In case you're interested to have a resource harvested into the INSPIRE GeoPortal you best contact the national contact point to understand the INSPIRE practices in your country. A list of national contact points is available at https://inspire.ec.europa.eu/contact-points/57734. A quick look into the contents of the portal and the harvest status page indicates that countries have quite varying practices on accepting datasets in the geoportal.</p>"},{"location":"tools/inspire-geoportal/#metadata-guidelines","title":"Metadata Guidelines","text":"<p>The TG Metadata indicates that INSPIRE datasets and services are described using ISO19139:2007. Discussion is ongoing if the allowed encodings should be extended to include for example DCAT (used by European data portal) or DataCite (used by Zenodo, dataverse, etc). </p>"},{"location":"tools/inspire-geoportal/#describe-datasets-using-iso191392007","title":"Describe datasets using iso19139:2007","text":"<p>Various tools exist to support describing datasets using iso19139:2007.</p> <ul> <li>GeoNetwork is a webbased application presenting forms and tools to assist in describing datasets.</li> <li>ArcMap and QGIS include an embedded metadata editor, which is able to export to iso19139:2007 </li> <li>pygeometa is a python library which exports ISO19139:2007 (and other metadata encodings) from a YML encoded format called <code>metadata control file</code> (mcf)</li> </ul>"},{"location":"tools/inspire-geoportal/#validate-records-in-the-inspire-validator","title":"Validate records in the INSPIRE validator","text":""},{"location":"tools/inspire-geoportal/#validate-links-in-the-link-checker","title":"Validate links in the Link Checker","text":"<p>Operational links are an important aspect in metadata. It determines for example if a user is able to view or download a file after having discovered its metadata. The INSPIRE Geoportal applies some link checks while harvesting the metadata and adds the link check result as tags to the metadata. To optimize your links beforehand the GeoPortal offers the option to run the link checker on an arbitrary metadata record. The Link checker is available at https://inspire-geoportal.ec.europa.eu/linkagechecker.html.</p>"},{"location":"tools/inspire-geoportal/#multilingual-metadata","title":"Multilingual metadata","text":"<p>Metadata harvested from a national catalogue is usually available in a local language only. To offer users a better user experience, the INSPIRE geoportal translates some key elements of the metadata to english. An automated service is used, which in some cases gives a unexpected translation result. For this reason we encourage you to use the multilingual options of ISO19139:2007 to provide the metadata at least in a local language and english.</p>"},{"location":"tools/ldproxy/","title":"ldproxy","text":"<p>Status: ready</p> <p>ldproxy is an open source product by interactive instruments. The team has an important role in the development of the suite of new OGC API's and the implementation of ldproxy is an important aspect of that process.</p>"},{"location":"tools/ldproxy/#quick-start","title":"Quick start","text":"<ul> <li>Run the image</li> </ul> <p><code>docker run -p7080:7080 -v${PWD}:/ldproxy/data iide/ldproxy</code></p> <ul> <li>Navigate to https://localhost:7080/manager/</li> <li>Login as usr:admin pwd:admin (set new password)</li> <li>You arrive in the 'services' page, create a new service with <code>plus</code> button top right</li> <li>We're setting up LDProxy to act as a proxy to provide OGC API Features over an existing WFS. Select type <code>WFS</code></li> <li>Provide a name and a WFS url (for example https://maps.isric.org/mapserv?map=/map/wosis_latest.map&amp;request=getcapabilities&amp;service=wfs)</li> <li>Click <code>ADD</code>. You return to the list of services, select the one you've just created and click on the <code>home</code> button top right to open it.</li> <li>Click <code>Access the data</code>, select a <code>collection</code> to visualise the items of the collection.</li> </ul>"},{"location":"tools/ldproxy/#read-more","title":"Read more","text":"<p>Github: https://github.com/interactive-instruments/ldproxy Docker: https://hub.docker.com/r/iide/ldproxy Documentation: https://docs.ldproxy.net/</p>"},{"location":"tools/link-checker/","title":"INSPIRE Link Checker","text":"<p>Status: in progress</p> <p>A tool to verify links in metadata</p> <ul> <li>Website: https://inspire-geoportal.ec.europa.eu/linkagechecker.html</li> </ul>"},{"location":"tools/mapserver/","title":"Mapserver","text":"<p>Status: ready</p> <p>Mapserver, originally <code>UMN Mapserver</code>, is an open source server component which provides OWS services on a variety of data sources. Mapserver is commonly used to set up INSPIRE View Services. A detailed guidance on how to use Mapserver to set up INSPIRE View Services is available at https://mapserver.org/ogc/inspire.html.</p> <p>MapServer supports WFS and WCS as data exchange mechanisms. Mapserver is not able to publish datasets having a hierarchical structure, as common in many INSPIRE datasets, which makes MapServer less suitable to provide INSPIRE download Services using WFS. Stored queries are supported. MapServer can be used to set up a WCS Download service.</p> <p>Mapserver runs as a CGI executable. The progream will start up as soon as a request arrives at the server. This makes mapserver very suitable for situations where many datasets are incidentally queried and scales out very well.</p> <p>Mapserver is configured using map files. These mapfiles contain metadata for each layer, connection details to the datasource and styling rules for the vizualisation. Various tools exist which create mapfiles automatically, from for example a QGIS layer with GeoCat Bridge. Or by using python script, for example with the mappyfile library. The View services relevant for INSPIRE Soil are described in INSPIRE Data Specification on Soil \u2013 Technical Guidelines in chapter 11. 3 types of layers can be distinguished: - Soil body, Soil profile and Soil Site are vector datasets indicating the location of research area's. - Soil properties as vector provide a map view of soil observations on soil profiles or the distribution of a soil property in soil bodies, derived from observations in the area and/or expert judgement. - Soil properties as coverage, coverage (grid) is a common output of statistical models which calculate the distribution of a soil property.</p>"},{"location":"tools/mapserver/#the-mapserver-mapfile","title":"The Mapserver Mapfile","text":"<p>For this recipe we'll prepare a WMS view service on a Soil Body dataset. For each Soil Body some derived soil properties of the top soil are available.</p> <p>Mapserver is configured using map files. These mapfiles contain metadata for each layer, connection details to the datasource and styling rules for the vizualisation. In a typical configuration a user 'calls' the mapserver executable via the web, while indicating the relevant mapfile. For example: </p> <pre><code>https://example.com/mapserv.cgi?map=/data/soilbody.map&amp;service=WMS&amp;request=GetCapabilities\n</code></pre> <p>Various tools exist which create mapfiles automatically, from for example a QGIS layer. See https://plugins.qgis.org/plugins/geocatbridge/.</p> <p>In this case we'll assemble the mapfile in a text editor. For some of the more advanced text editors, such as Visual Studio Code, mapfile editing plugins are available, which provide validation and syntax highlighting.</p> <p>A generic mapfile Quick Start is provided at https://live.osgeo.org/en/quickstart/mapserver_quickstart.html. The quickstart is based on OSGEO Live, a virtual DVD, which offers a preinstalled mapserver and has data from Natural Earth. </p> <p>Within the mapfile, created in the Quickstart, let's replace some metadata and update the natural earth layer to point to our soil body dataset.</p> <pre><code>MAP\n  NAME \"SOILBODY_QUICKSTART\"\n  EXTENT -180 -90 180 90\n  UNITS DD\n  SHAPEPATH \"/home/user/data/\"\n  SIZE 800 600\n\n  IMAGETYPE PNG24\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    METADATA\n      ows_title \"Soil Body Quickstart\"\n      ows_enable_request \"*\"\n      ows_srs \"EPSG:4326 EPSG:25832 EPSG:25833\"\n    END\n  END\n\n  LAYER\n    NAME \"Soilbody\"\n    STATUS ON\n    TYPE POLYGON\n    DATA \"soilbody\"\n    CLASS\n      STYLE\n        COLOR 246 241 223\n        OUTLINECOLOR 0 0 0\n      END\n    END\n  END\n\nEND\n</code></pre>"},{"location":"tools/mapserver/#mapserver-via-docker","title":"Mapserver via Docker","text":"<p>Mapserver requires a number of dependencies, which may be hard to install on some systems, that's why this recipe suggests to work with Docker containers which are prepared to run mapserver.</p> <p>The camp2camp mapserver image is a commonly used mapserver container image. While starting the container we provide a number of parameters so the container is able to locate the mapfile and the data files.</p> <pre><code>docker run -p=80:80 \\\n    -v=$(PWD)/soilbody.map:/etc/mapserver/wms.map \\\n    -v=$(PWD)/data:/home/user/data/ \\\n    camptocamp/mapserver\n</code></pre>"},{"location":"tools/mapserver/#testing-a-mapfile","title":"Testing a mapfile","text":"<p>Sometimes mapserver reports an error while generating a map response. If the logs of the container do not provide enough information, mapserver provides an interesting utility map2img to validate a mapfile. To run the utility, you have to open bash on the container: </p> <pre><code>docker exec -it &lt;container&gt; /bin/bash \n</code></pre> <p>and then run the utily:</p> <pre><code>map2img -m local.map -o test2.png\n</code></pre> <ul> <li>-m references the mapfile</li> <li>-o references an output file to be generated</li> </ul>"},{"location":"tools/mapserver/#wrb-layer-with-geocat-bridge","title":"WRB layer with GeoCat Bridge","text":"<p>The technical guidance provides quite detailed instructions on how to style the relavant soil layers. For example a SO.SoilBody.WRB is described with dedicated colors for each WRB Soil type.</p> WRB RSG Code WRB RSG Name Colour RGB code Colour HEX code AC Acrisol (247, 152, 4) #F79804 AB Albeluvisol (254, 194, 194) #FEC2C2 AL Alisol (255, 255, 190) #FFFFBE .. .. .. .. <p>If many style rules are involved (or if your project already has styling) a tool like GeoCat Bridge is helpfull. Read more about GeoCat Bridge in the recipe Bridge and GeoServer. On QGIS, with the GeoCat Bridge plugin installed, load a SoilBody dataset and assign some of the colors. In the web &gt; bridge menu, activate the Style viewer panel. Notice the various tabs in the panel which represent the layer style in various encodings. The first tab contains Styled Layer Descriptor (SLD), a standardised styling format, used for example in GeoServer. The second tab presents the mapfile syntax, you can copy the value into your mapfile (or let Bridge generate a full mapfile).</p> <p></p>"},{"location":"tools/mapserver/#mapserver-and-wmts","title":"Mapserver and WMTS","text":"<p>Mapserver does not provide tile services (WMTS) itself, but is often combined with a separate tool, mapcache, which provides tile service on top of a MapServer instance. Tile services are generally a safer option with respect to Quality of Service, but less dynamic in update and styling options. An interesting option is to use the WMS option of Mapcache, which uses a cache of tiles as a source to provide WMS services.</p>"},{"location":"tools/mapserver/#read-more","title":"Read more:","text":"<ul> <li>Website: https://mapserver.org</li> <li>Github: https://github.com/MapServer/MapServer</li> <li>Docker: https://hub.docker.com/r/camptocamp/mapserver</li> <li>OSGeo: https://www.osgeo.org/projects/mapserver</li> </ul>"},{"location":"tools/postgraphile/","title":"GraphQL with Postgraphile","text":"<p>Status: in progress</p> <p>GraphQL is a defacto standard for self describing api's on hierarchical data. Graphql provides via its api specification capabilities to query datasets using filters, but also indicate which properties to be returned. The GraphQL website provides a quick start on Graphql based on NodeJS.</p> <p>GraphQL is currently not endorsed as an INSPIRE Good Practice, but it fits many aspects of an INSPIRE download service. GraphQL is a good fit to disseminate measurement and observation data from soil profiles.</p> <p>At ISRIC we use GraphQL (along WMS/WFS) to disseminate the WOSIS soil profile database.</p>"},{"location":"tools/postgraphile/#postgraphile","title":"Postgraphile","text":"<p>Postgraphile is a NodeJS server application which creates a GraphQL api on any postgres database. For this recipe we've prepared a docker orchestration which includes a postgres database and a postgraphile container. Also included is PGadmin, which provides a webbased user interface to the database.</p>"},{"location":"tools/postgraphile/#load-some-data-into-the-database","title":"Load some data into the database","text":""},{"location":"tools/postgraphile/#query-a-graphql-endpoint","title":"Query a GraphQL endpoint","text":"<p>Website: https://www.graphile.org/postgraphile/</p>"},{"location":"tools/pycsw/","title":"pycsw","text":"<p>Status: in progress</p> <p>An open source python catalogue implementation, used within CKAN-spatial and GeoNode.</p> <p>Supports CSW (v2 and v3) and OGC API Records. Uses a fixed metadata model and has various metadata model output formats. Capability to harvest metadata from remote sources (CSW, WMS, WFS, SOS, etc)</p>"},{"location":"tools/pycsw/#quick-start","title":"Quick start","text":"<pre><code>docker run -d -name pycsw -p 8000:8000 geopython/pycsw\n</code></pre> <ul> <li>Visit https://localhost:8000</li> <li>Prepare a folder of iso19139 files, mount it on the container and load it with pycsw-admin.py</li> </ul>"},{"location":"tools/pycsw/#read-more","title":"Read more","text":"<ul> <li>Website: https://pycsw.org/</li> <li>Github: https://github.com/geopython/pycsw</li> <li>Docker: https://hub.docker.com/r/geopython/pycsw</li> <li>Demo: https://demo.pycsw.org/cite/collections</li> </ul>"},{"location":"tools/pygeoapi/","title":"OGC API Features with pygeoapi","text":"<p>Status: in progress</p> <p>A good practice has been published on how to provide an INSPIRE dowload service based on OGC API Features. In this recipe we'll set up an instance of pygeoapi with some soil data. pygeoapi is an open source python server implementation of OGCAPI Features, Tiles, Coverages, Records and Processes.</p> <p>The geopython community has prepared a workshop on getting started with pygeoapi.</p>"},{"location":"tools/pygeoapi/#quick-start","title":"Quick start","text":"<p>The recipe is based on Docker. New to docker? Read more in the Docker recipe.</p> <ul> <li>With commandline in a new folder, run this command:</li> </ul> <pre><code>docker run -p5000:80 geopython/pygeoapi:latest\n</code></pre> <ul> <li>Navigate with your browser to http://localhost:5000 </li> </ul> <p>If all went fine, you now see the default pygeoapi installation with sample data. In the next step we'll publish a new soil dataset. pygeoapi's configuration is stored in a config file. The config file is encoded as YAML. The first part configures the main settings of the service, in the second part individual datasets are configured.</p> <ul> <li>Download the Dutch INSPIRE dataset 'soil drills' from https://service.pdok.nl/bzk/brobhrpvolledigeset/atom/v1_1/downloads/brobhrpvolledigeset.zip</li> <li>Unzip the file to the work folder</li> <li>Create a local config file in the folder, download it from here</li> <li>Remove all the datasets from the config folder, and replace it for the following:</li> </ul> <pre><code>resources:\n    bro:\n        type: collection\n        title: BRO\n        description: Bro soil drills\n        keywords:\n            - soil\n        links:\n            - type: application/geopackage+sqlite\n              rel: canonical\n              title: source data\n              href: https://service.pdok.nl/bzk/brobhrpvolledigeset/atom/v1_1/downloads/brobhrpvolledigeset.zip\n              hreflang: en-US\n        extents:\n            spatial:\n                bbox: [-180,-90,180,90]\n                crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\n        providers:\n            -   type: feature\n                name: OGR\n                data:\n                    source_type: GPKG\n                    source: /pygeoapi/brobhrpvolledigeset.gpkg\n                    gdal_ogr_options:\n                        SHPT: POINT\n                id_field: bro_id\n                layer: borehole_research\n</code></pre> <ul> <li>Mount config file in containter</li> </ul> <pre><code>docker run -p5000:80 -v ${PWD}/pygeoapi-config.yml:/pygeoapi/local.config.yml -v ${PWD}/brobhrpvolledigeset.gpkg:/pygeoapi/brobhrpvolledigeset.gpkg  geopython/pygeoapi:latest\n</code></pre>"},{"location":"tools/pygeoapi/#validation","title":"Validation","text":"<p>JRC recently extended the INSPIRE validator. It can now also validate an OGC API Features service. Because docker runs locally, you need to set up a tunnel for the validator to access the local service. Read the tunnel recipe to see how to do that.</p>"},{"location":"tools/pygeoapi/#read-more","title":"Read more","text":"<ul> <li>Website: https://pygeoapi.io/</li> <li>Github: https://github.com/geopython/pygeoapi</li> <li>Docker: https://hub.docker.com/r/geopython/pygeoapi</li> <li>Demo: https://demo.pygeoapi.io/master</li> <li>Documentation: https://docs.pygeoapi.io/en/latest</li> <li>OSGeo: https://www.osgeo.org/projects/pygeoapi </li> </ul>"},{"location":"tools/pygeometa/","title":"A pythonic metadata workflow","text":"<p>status: in progress</p> <p>This recipe presents a minimalistic, however integrated and standardised approach to metadata management. Each data file on the file system will be accompagnied by a minimal YML metadata file. Automated pipelines will pick up these metadata files and publish them as valid iso19139 (or other metadata models) documents.</p> <p>The recipe uses an incremental approach to extend the functionality of the data management system step by step.</p>"},{"location":"tools/pygeometa/#quick-start","title":"Quick start","text":"<p>The inital step assumes a folder of data files on a network drive, sharepoint or git repository. Many datasets are stored on a database, we'll not consider these for now.</p> <p>For each data file in the folder we will create a <code>metadata control file</code> (mcf). Mcf is a convention from the pygeometa community. It is a yaml encoded subset of iso19139:2007. YAML is easy to read by humans and optimal for versioning.</p> <pre><code>pip install pygeometa\n</code></pre> <pre><code>pygeometa export \n</code></pre> <p>Idea of this approach is to manually capture as little metadata as needed for a file, auto derive as much as possible from the spatial file itself (format, extent, crs) and merge with higher level metadata to capture generic aspects (contact details, usage constraints, etc.)</p> <p>we've prepared a docker image including python scripts to work efficiently with pygeometa</p> <pre><code>docker run /isric/\n</code></pre> <p>Publish records on a WebDav location or publish them for example in pycsw</p>"},{"location":"tools/qgis/","title":"View services with QGIS server","text":"<p>Status: contributions required</p> <p>QGIS started as a GIS desktop application. In recent years the community prepared a server edition of QGIS. The server provides WMS, WFS and since recently OGC API Features. </p> <p>The advantage of using QGIS both as a desktop and server component is that the maps generated on the server will be exactly the same as those prepared on a desktop client.</p>"},{"location":"tools/qgis/#prepare-a-map-service-in-qgis-desktop","title":"Prepare a map service in QGIS Desktop","text":"<p>Name your project project.qgs</p> <p>On project settings, open the WMS properties to add relevant metadata</p>"},{"location":"tools/qgis/#publish-a-map-service","title":"Publish a map service","text":"<p>We're running QGIS server as a docker container based on https://hub.docker.com/r/camptocamp/qgis-server</p> <pre><code>docker run -p 8080:80 --volume=$PWD:/etc/qgisserver camptocamp/qgis-server\n</code></pre> <p>Try the service via</p> <pre><code>http://localhost:8080/?SERVICE=WMS&amp;REQUEST=GetCapabilities\n</code></pre>"},{"location":"tools/qgis/#qgis-desktop-as-a-client-for-inspire-services","title":"QGIS Desktop as a client for INSPIRE services","text":"<p>It can act as a client for WMS, WMTS, WCS, Sensorthings API and OGC API Features.</p> <p>QGIS data model is based on data layers and is therefore less optimal for hierachical vector data as used in INSPIRE. There have been initiatives to bring hierarchical data to QGIS, such as GMLAS but adoption has been limited. The Application Schema functionality has been introduced in GDAL/OGR, the QGIS plugin builds on top of it. OGR converts the contents of the GML to a relational database. Individual tables from that database are loaded in the QGIS interface.</p>"},{"location":"tools/rasdaman/","title":"Coverages with rasdaman","text":"<p>Coverage data is the digital representation of some spatio-temporal phenomenon. Usually in the form of a grid of cells having a certain resolution. Grid cells can provide a value for multiple phenomena. The OGC Web Coverage Service and the upcoming OGC API Coverages provide a standardised mechanism to query coverage data over the web. Web Coverage Service is the main option to provide a Download service on INSPIRE coverage data. An alternative option is INSPIRE Atom.</p> <p>Rasdaman is software to set up a Web Coverage Service. The Rasdaman team has prepared a tutorial for setting up an INSPIRE Coverage service at Good practice on coverage data. The tutorial is set up using Jupyter. New to Jupyter? Read the Jupyter recipe.</p>"},{"location":"tools/rasdaman/#read-more","title":"Read more:","text":"<ul> <li>Website: http://rasdaman.org</li> <li>Docker: https://github.com/ARPASMR/rasdaman</li> <li>Sources: http://www.rasdaman.org/browser</li> <li>Tickets: http://www.rasdaman.org/report/1</li> <li>OsGeo: https://www.osgeo.org/projects/rasdaman/</li> </ul>"},{"location":"tools/re3gistry/","title":"Re3gistry","text":"<p>Status: contributions required</p> <p>A system for publishing code lists</p>"},{"location":"tools/re3gistry/#read-more","title":"Read more","text":"<ul> <li>Github: https://github.com/ec-jrc/re3gistry</li> <li>Manual: https://github.com/ec-jrc/re3gistry/blob/master/documentation/user-manual.md</li> <li>ISA2: https://ec.europa.eu/isa2/solutions/re3gistry_en/</li> </ul>"},{"location":"tools/rml/","title":"RML.io","text":"<p>RML.io is a toolset for the generation of knowledge graphs. They automate the creation of RDF from diverse data sources, primarily unstructured tabular data. </p> <p>RML.io has programmes to be used on-line and to be installed on computer systems (Linux, MacIntosh and Windows platforms are supported). The former are useful for prototyping, whereas the latter are meant for actual transformations of large datasets.</p>"},{"location":"tools/rml/#install","title":"Install","text":"<p>Using RML.io in your system requires two programmes, a parser for the YARRRML syntax (<code>yarrrml-parser</code>) and a transformer that converts tabular data to RDF (<code>rmlmapper</code>).</p> <p>The first of these programmes is installed with <code>npm</code>:</p> <pre><code>npm i -g @rmlio/yarrrml-parser\n</code></pre> <p><code>rmlmapper</code> is a Java programme, that can be downloaded directly from the project GitHub page. For instance:</p> <pre><code>wget https://github.com/RMLio/rmlmapper-java/releases/download/v6.1.3/rmlmapper-6.1.3-r367-all.jar\n</code></pre> <p>It can then be run with the Java Runtime Environment:</p> <pre><code>java -jar rmlmapper-6.1.3-r367-all.jar\n</code></pre> <p>At this stage it might be useful to create a shortcut to call the programme with a simple command like <code>rmlmapper</code>. How to do this depends on your system and is beyond the scope of this document.</p>"},{"location":"tools/rml/#the-yarrrml-syntax","title":"The YARRRML syntax","text":"<p>The RML tools apply data transformations according to a set of rules recorded in a YAML file. This file must respect a specific syntax, named YARRRML. This specification defines a number of sections (or environments) in the YAML file that lay out the structure of the resulting triples.</p> <p>The first of these sections is named <code>prefixes</code> and provides the space for the definition of URI abbreviations, in all similar to the Turtle syntax. Each abbreviation is encoded as a list item and can be used in the reminder of the YARRRML as it would be in a Turtle knowledge graph.</p> <pre><code>prefixes:\n rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n xsd: http://www.w3.org/2001/XMLSchema#\n geo: http://www.opengis.net/ont/geosparql#\n</code></pre> <p>Next comes the <code>mappings</code> section, where the actual transformations are encoded. This section is to be populated with sub-sections, one for each individual subject class (or type) necessary in the output RDF. For instance, if the transformation must produce triples for profiles and layers, then a sub-suction for each is necessary. The name of these subject sub-sections is arbitrarily chosen by the user.</p> <pre><code>mappings:\n  profile:\n\n  layer:\n</code></pre> <p>For each subject class sub-section at least one data source needs to be specified in the <code>sources</code> section. The source can be declared within square brackets (i.e. a YAML collection), providing a path to a file followed by a tilde and then a type. The sources section can be more intricate, as YARRRML supports a wide range of different data sources, including flat tables, databases and Web APIs. </p> <pre><code>mappings:\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\n</code></pre> <p>The following sub-section of the class declares the subject and has the simple name of <code>s</code>. Its purpose is to define the URI structure for the instances of the class. In principle this is also the first element that makes reference to the contents of the source file. In the case of CSV, as in this example, the column names are used. They are invoked using the dollar character (<code>$</code>), with the column name within parenthesis. The practical result is the generation of an individual element (subject in this case) for each distinct value found in the source column. </p> <pre><code>  profile:\n    sources:\n      - ['SoilData.csv~csv']\n    s: http://my.soil.org#$(profile_id)\n</code></pre> <p>With the subject defined, triples can be completed with predicates and objects in sub-section <code>po</code>. This section is itself composed by a list, whose items comprise a pair: predicate (item <code>p</code>) and object (item <code>o</code>). The predicate is encoded as a URI in a similar way to the subject, using abbreviations if necessary. As for the object it can be decomposed further into a <code>value</code> and a <code>datatype</code> to accommodate literals.   </p> <p>The example below creates triples for the layer class subject, using the <code>layer_id</code> column in the source to generate subject URIs. The source column <code>layer_order</code> is used to complete triples declaring the order of a layer within a profile.</p> <pre><code>prefixes:\n xsd: http://www.w3.org/2001/XMLSchema#\n iso28258: http://w3id.org/glosis/model/iso28258/2013#\n\nmappings:\n  layer:\n    sources:\n      - ['SoilData.csv~csv']\n    s: http://my.soil.org#$(layer_id)\n    po:\n      - p: iso28258:ProfileElement.order\n        o:\n           value: \"$(layer_order)\"\n           datatype: xsd:integer\n</code></pre> <p>The encoding of the predicates and objects list can be shortened with collections. Instead of discriminating value and datatype, they can be expressed as elements of a collection. This formulation is useful when the object is itself a URI. Note how in the example below (for the layer class) the tilde is used again, to indicate the object type. </p> <pre><code>    po:\n      - [iso28258:Profile.elementOfProfile, http://my.soil.org#$(layer_id)~iri]\n</code></pre> <p>This was just a brief introduction to the YARRRML syntax. It goes far deeper, even allowing for some functional programming. While the guidelines in this document make enough of a start to automated RDF generation, the documentation is indispensable to take full advantage of the RML tool set.</p>"},{"location":"tools/rml/#how-to-use","title":"How to use","text":"<p>The file SoilData.csv contains a simple set of hypothetical measurements referring to three soil profiles collected in two different sites. The goal is to transform this dataset into GloSIS compliant RDF.</p> <pre><code>site_id,lat,lon,profile_id,layer_id,upper_depth,lower_depth,pH,SOC,\n1,49.43,8.31,1,11,0,15,7.4,6,\n1,49.43,8.31,1,12,15,40,7.2,4,\n1,49.43,8.31,2,21,0,10,8,3,\n1,49.43,8.31,2,22,10,30,8.1,2,\n2,46.82,11.45,3,31,0,15,6.8,1,\n2,46.82,11.45,3,32,15,30,6.7,1,\n2,46.82,11.45,3,33,30,60,6.7,0,\n</code></pre>"},{"location":"tools/rml/#selecting-a-uri-structure","title":"Selecting a URI structure","text":"<p>Before starting a data transformation into RDF you must devise an appropriate URI structure. Every non-literal element in the resulting knowledge graph must have a URI, its unique identifier on the World Wide Web.</p> <p>A simple approach is to use a sub-domain of your institutional domain to identify a single project or dataset. Then concatenate the name of the OWL class to which the data instance belongs, and finally compound a unique identifier of the instance (the latter should already exist if you use a relational database). The template for this approach looks like:</p> <pre><code>http://project.institution.org/class#identifier\n</code></pre> <p>This example applies the URIs currently in use for the World Soil Information Service (WoSIS):   - http://wosis.isric.org/site#   - http://wosis.isric.org/profile#   - http://wosis.isric.org/layer#   - http://wosis.isric.org/observation#   - http://wosis.isric.org/result#</p> <p>Note that none of the data used in this exercise are actually part of WoSIS.</p>"},{"location":"tools/rml/#profiles","title":"Profiles","text":"<p>The simplest place to start is the profiles. There are three essential elements to generate for each profile:   - A new URI for the profile;   - The declaration of the new profile as an instance of the class <code>GL_Profile</code>;   - The association with the respective site. </p> <p>Below are the contents of the file profile.yarrrml that encodes this transformation. Note how the URIs of both the profile and the site are created using the prefixes.</p> <pre><code>prefixes:\n wosis_prf: http://wosis.isric.org/profile#  \n wosis_sit: http://wosis.isric.org/site#\n glosis_pr: http://w3id.org/glosis/model/profile# \n iso28258: http://w3id.org/glosis/model/iso28258/2013#\n\nmappings:\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_prf:$(profile_id)\n    po:\n      - [a, glosis_pr:GL_Profile]\n      - [iso28258:Profile.profileSite, wosis_sit:$(site_id)~iri]\n</code></pre> <p>To perform the actual transformation you must first apply <code>yarrrml-parser</code> to create the RML transformation file and then use <code>rmlmapper</code> to obtain the actual knowledge graph. Be default <code>rmlmapper</code> creates a Turtle file that is printed to the standard output (STDOUT). You can use the parameters <code>-o</code> to redirect output to a text file and <code>-s</code> to select an alternative serialisation syntax.</p> <pre><code>yarrrml-parser -i profile.yarrrml -o profile.rml.ttl\nrmlmapper -s turtle -m profile.rml.ttl\n</code></pre>"},{"location":"tools/rml/#sites","title":"Sites","text":"<p>Sites are the spatial features in the knowledge graph, therefore they require the creation of appropriate GeoSPARQL instances. Three new elements must be addressed in this transformation:   - Declaration of the site as an instance of the class <code>geo:Feature</code>;   - Creation of a <code>geo:Geometry</code> instance to host the actual geo-spatial     information;   - A literal of the type <code>geo:wktLiteral</code> or <code>geo:gmlLiteral</code> to encode the geometry.</p> <p>The file site.yarrrml achieves this transformation. Its contents are reproduced below:</p> <pre><code>prefixes:\n geo: http://www.opengis.net/ont/geosparql#\n glosis_sp: http://w3id.org/glosis/model/siteplot# \n wosis_sit: http://wosis.isric.org/site#\n wosis_geo: http://wosis.isric.org/geometry#\n\nmappings:\n  site:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_sit:$(site_id)\n    po:\n      - [a, glosis_sp:GL_Site]\n      - [a, geo:Feature]\n      - [geo:hasGeometry, wosis_geo:$(site_id)~iri]\n\n  geometry:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_geo:$(site_id)\n    po:\n      - [a, geo:Point]\n      - p: geo:asWKT\n        o:\n           value: \"POINT($(lon) $(lat))\"\n           datatype: geo:wktLiteral\n</code></pre> <p>This example also shows the inclusion of two different classes in the same transformation. Note how the Feature is associated with the geometry using the <code>geo:hasGeometry</code> object property. Also important is the creation of the WKT literal, as it requires a verbose declaration of the object to make the type explicit.</p>"},{"location":"tools/rml/#layers","title":"Layers","text":"<p>Having created a transformation for the sites, one for the layers is not much of a challenge. Download the file layer.yarrrml and try it yourself. </p> <p>Look carefully at the transformation file, note how the object properties from the ISO28258 module are used to declare the layers depths.</p> <p>Question: what would be different if in the source dataset horizons were identified instead of layers? </p>"},{"location":"tools/rml/#measurements","title":"Measurements","text":"<p>An example transformation for the measurements in the original dataset is available in the file measurements.yarrrml. The extra elements to address in this transformation are:   - Instance of the respective Observation class;   - Instance of the respective Result class;   - Relation between Observation and Result;   - Numerical literal with the measurement result.</p> <p>Question: Identify in the Layer Horizon module of GloSIS which are the units of measurement associated with the Result instances used in this example. </p> <p>Exercise I: Create a new <code>yarrrml</code> file including all the transformations given above, creating all necessary triples for sites, profiles, layers and measurements. Make sure it is correctly parsed by <code>yarrrml-parser</code> and generate a new, complete, knowledge graph.</p> <p>Exercise II: Modify the transformation you obtained in the previous exercise so that it declares all pH measurements as resulting from a H2O procedure (water solution).</p>"},{"location":"tools/sql/","title":"SQL &amp; python","text":"<p>Status: in progress</p> <p>This recipe implements the GeoPackage Good Practice for Soil Data by converting a sample soil database to GeoPackage based on the INSPIRE Soil Model using SQL statements from a Python environment.</p> <p>REQUIREMENTS - basic understanding of python and sql concepts - running environment python, pip, virtualenv (or alternative)</p> <p>Setup a python environment</p> <pre><code>virtualenv soildata\ncd soildata\n. bin/activate\n</code></pre> <p>Clone the workshop environment and install python requirements</p> <pre><code>git clone \npip install -r requirements.txt\n</code></pre> <p>As a soil database we use the SOTER database of Cuba which is available as Microsoft Access mdb as well as SQLite. </p>"},{"location":"tools/validator/","title":"INSPIRE validator","text":"<p>Status: in progress</p> <p>A validator for INSPIRE services and data</p>"},{"location":"tools/validator/#exercise","title":"Exercise","text":"<p>In this recipe we're testing (and improving) an INSPIRE Soil dataset.</p> <p>The docker container runs locally, so it can not be tested by the INSPIRE Validator.  In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.</p>"},{"location":"tools/validator/#read-more","title":"Read more","text":"<p>Website: https://inspire.ec.europa.eu/validator/home/index.html Github: https://github.com/INSPIRE-MIF/helpdesk-validator</p>"},{"location":"tools/virtuoso/","title":"Virtuoso &amp; Skosmos","text":"<p>Status: in progress</p> <p>This recipe presents steps to publish a soil codelists using the SKOS ontology in Virtuoso and Skosmos. Virtuoso is an open source Triple store providing a SPARQL endpoint.  Skosmos is an open source web application providing a human friendly browse interface for skos thesauri stored in a triple store.</p> <p>The skos ontology supports the use of knowledge organization systems such as thesauri,  classification schemes, subject heading lists and taxonomies within the framework of the Semantic Web (and beyond). For any concept (identified by a uri) a definition in multiple languages is modeled. For each concept a range of relation types is available to capture it's relation to other concepts, e.g. SameAs, Broader, Narrower, Affect, hasComponent, etc.</p>"},{"location":"tools/virtuoso/#preparing-the-codelist","title":"Preparing the codelist","text":"<p>Excel is often used to create lists of concepts with their definition. But consider that tabular systems are less optimal when defining relations between concepts. For that reason the Glosis codelists are maintained as Turtle RDF files on Github.</p> <p>For this recipe we assume the codelist to be published is available as a CSV. A range of tools is available to transform an excel to a skos RDF document. Skos play offers for example a webbased conversion tool.</p>"},{"location":"tools/virtuoso/#load-skos-rdf-to-virtuoso","title":"Load SKOS RDF to virtuoso","text":"<p>We're using a docker compose orchestration to deploy virtuoso and skosmos locally. Copy the contents of the virtuoso folder into an empty folder. Navigate to the folder with command line and run:</p> <pre><code>docker compose up\n</code></pre> <p>Load RDF data into Virtuoso ...</p>"},{"location":"tools/virtuoso/#setup-skosmos","title":"Setup Skosmos","text":"<p>The file config-docker.ttl contains the configuration of SKOSMOS. You have to indicate which elements to select from the triple store.</p>"},{"location":"tools/virtuoso/#extending-an-existing-codelist","title":"Extending an existing codelist","text":""},{"location":"tools/virtuoso/#read-more","title":"Read more","text":"<p>Virtuoso</p> <ul> <li>Website: https://virtuoso.openlinksw.com/</li> <li>Github: https://github.com/openlink/virtuoso-opensource</li> <li>Docker: https://hub.docker.com/r/openlink/virtuoso-opensource-7</li> <li>Virtuoso at ISRIC: https://virtuoso.isric.org/</li> </ul> <p>Skosmos</p> <ul> <li>Website: https://skosmos.org</li> <li>Github: https://github.com/NatLibFi/Skosmos</li> <li>Docker: https://hub.docker.com/r/ndslabs/skosmos</li> <li>Skosmos examples: https://glosis.isric.org/, https://agrovoc.fao.org, https://agclass.nal.usda.gov/</li> </ul> <p>Skos play</p> <ul> <li>Skos play: https://skos-play.sparna.fr/play/convert</li> </ul>"},{"location":"tools/webdav/","title":"WebDav &amp; Atom","text":"<p>Status: ready</p> <p>In the technical Guidelines download services <code>INSPIRE atom services</code> are described to provide a light weight alternative to WFS and WCS, while fitting with all the aspects of a <code>webservice</code> as described in the implementing rules. </p> <p>This recipe describes a minimal approach which is based on placing a number of Atom-xml files along side the downloadable resources in a web accessible folder or webdav. Consider that WebDav is used as an example, any online file system would suffice. A plain apache webserver, Zenodo or even sharepoint or dropbox.</p>"},{"location":"tools/webdav/#wsgidav","title":"WSGIDAV","text":"<p>Various webdav implmentations exist; apache webdav, NGINX DAV, SFTPGO, wsgidav. For this recipe we'll use wsgidav, but others will work in a similar way.</p> <p>WsgiDAV provides a docker image, with below statement you advertise the current folder via WebDAV.</p> <pre><code>docker run --rm -it -p 8080:8080 -v ${PWD}:/var/wsgidav-root mar10/wsgidav\n</code></pre> <p>Open http://localhost:8080 in your browser to see the file contents.</p>"},{"location":"tools/webdav/#atom-files","title":"ATOM files","text":"<p>Stop the container (ctrl-C). Create a new folder and copy your dataset(s) of choice into it. Create a file <code>service.atom.xml</code> and for each dataset a new text file with the same name, but the atom extension.</p> <p>service.atom.xml is the <code>service feed</code> (comparable to the capabilities operation in OWS). The service feed will contain details about the service and link to each of the dataset feeds. Populate the service feed with (replace relevant sections):</p> <pre><code>&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"\n  xmlns:georss=\"http://www.georss.org/georss\" \n  xmlns:inspire_dls=\"http://inspire.ec.europa.eu/schemas/inspire_dls/1.0\" \n  xml:lang=\"en\"&gt;\n &lt;!-- feed title --&gt;\n &lt;title&gt;XYZ Example INSPIRE Download Service&lt;/title&gt;\n &lt;!-- feed subtitle --&gt;\n &lt;subtitle&gt;INSPIRE Download Service of Soil Properties data in Sahel region&lt;/subtitle&gt;\n &lt;!-- self-referencing link to this feed --&gt;\n &lt;link href=\"http://localhost:8080/service.atom.xml\" rel=\"self\" type=\"application/atom+xml\"  hreflang=\"en\" title=\"This document\"/&gt;\n &lt;!-- link to Open Search definition file for this servicen (not implemented) \n&lt;link rel=\"search\" href=\"http://example.com/search/opensearchdescription.xml\" type=\"application/opensearchdescription+xml\" title=\"Open Search Description for XYZ download service\"/&gt; --&gt;\n &lt;!-- identifier --&gt;\n &lt;id&gt;http://localhost:8080/service.atom.xml&lt;/id&gt;\n &lt;!-- rights, access restrictions --&gt;\n &lt;rights&gt;Copyright (c) 2021, XYZ; all rights reserved&lt;/rights&gt;\n &lt;!-- date/time this feed was last updated --&gt;\n &lt;updated&gt;2021-03-31T13:45:03Z&lt;/updated&gt;\n &lt;!-- author contact information --&gt;\n &lt;author&gt;&lt;name&gt;John Doe&lt;/name&gt;&lt;email&gt;doe@example.com&lt;/email&gt;&lt;/author&gt;\n &lt;category term=\"http://inspire.ec.europa.eu/metadata-codelist/SpatialDataServiceCategory/infoFeatureAccessService\" scheme=\"http://inspire.ec.europa.eu/metadata-codelist/SpatialDataServiceCategory\"/&gt;\n &lt;!-- entry for a \"Dataset Feed\" for a pre-defined dataset --&gt;\n &lt;entry&gt;\n    &lt;!-- title for \"Dataset Feed\" for pre-defined dataset --&gt;\n    &lt;title&gt;soil properties ABC Dataset Feed&lt;/title&gt;\n    &lt;!-- Spatial Dataset Unique Resource Identifier for this dataset--&gt;\n    &lt;inspire_dls:spatial_dataset_identifier_code&gt;wn_id1&lt;/inspire_dls:spatial_dataset_identifier_code&gt; \n    &lt;inspire_dls:spatial_dataset_identifier_namespace&gt;http://example.com/&lt;/inspire_dls:spatial_dataset_identifier_namespace&gt;\n    &lt;!-- link to dataset metadata record --&gt;\n    &lt;link href=\"http://example.com/metadata/abcISO19139.xml\" rel=\"describedby\" type=\"application/xml\"/&gt;\n    &lt;!-- link to \"Dataset Feed\" for pre-defined dataset --&gt;\n    &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties.atom.xml\" type=\"application/atom+xml\"  hreflang=\"en\" title=\"Feed containing the soil properties data\"/&gt;\n    &lt;!-- identifier for \"Dataset Feed\" for pre-defined dataset --&gt;\n    &lt;id&gt;http://localhost:8080/soilproperties.atom.xml&lt;/id&gt;\n    &lt;!-- rights, access info for pre-defined dataset --&gt;\n    &lt;rights&gt;Copyright (c) 2002-2021, XYZ; all rights reserved&lt;/rights&gt;\n    &lt;!-- last date/time this entry was updated --&gt;\n    &lt;updated&gt;2012-03-31T13:45:03Z&lt;/updated&gt;\n    &lt;!-- summary --&gt;\n    &lt;summary&gt;This is the entry for soil properties ABC Dataset&lt;/summary&gt;\n    &lt;!-- optional GeoRSS-Simple polygon outlining the bounding box of the pre-defined dataset described by the entry. Must be lat lon --&gt;\n    &lt;georss:polygon&gt;47.202 5.755 55.183 5.755 55.183 15.253 47.202 15.253 47.202 5.755&lt;/georss:polygon&gt;\n    &lt;!-- CRSs in which the pre-defined Dataset is available --&gt;\n    &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/4258\" label=\"ETRS89\"/&gt;\n &lt;/entry&gt;\n&lt;/feed&gt;\n</code></pre> <p>Notice that we're not implementing the actual opensearch search functionality yet. You can leave the <code>opensearchdescription</code> line empty for now. There are some external options to provide the opensearch, the Technical Guidance document actually includes a PHP script to facilitate opensearch.</p> <p>Then for each dataset add a file <code>soilproperties.atom.xml</code>:</p> <pre><code>&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"\n xmlns:georss=\"http://www.georss.org/georss\" xml:lang=\"en\"&gt;\n    &lt;!-- feed title --&gt;\n    &lt;title&gt;INSPIRE Dataset Soil properties Download&lt;/title&gt;\n    &lt;!-- feed subtitle --&gt;\n    &lt;subtitle&gt;INSPIRE Download Service, of organisation XYZ providing dataset Soil Properties&lt;/subtitle&gt;\n    &lt;!-- links to INSPIRE Spatial Object Type definitions for this predefined dataset --&gt;\n    &lt;link href=\"https://inspire.ec.europa.eu/featureconcept/SoilProfile\" rel=\"describedby\" type=\"text/html\"/&gt;\n    &lt;!-- self-referencing link to this feed --&gt;\n    &lt;link href=\"http://localhost:8080/soilproperties.atom.xml\" rel=\"self\" \n    type=\"application/atom+xml\"\n    hreflang=\"en\" title=\"This document\"/&gt;\n    &lt;!-- upward link to the corresponding download service feed --&gt;\n    &lt;link href=\"http://localhost:8080/service.atom.xml\" rel=\"up\" type=\"application/atom+xml\" hreflang=\"en\" title=\"The parent service feed document\"/&gt;\n    &lt;!-- identifier --&gt;\n    &lt;id&gt;http://localhost:8080/soilproperties.atom.xml&lt;/id&gt;\n    &lt;!-- rights, access restrictions --&gt;\n    &lt;rights&gt;Copyright (c) 2021, XYZ; all rights reserved&lt;/rights&gt;\n    &lt;!-- date/time this feed was last updated --&gt;\n    &lt;updated&gt;2021-03-31T13:45:03Z&lt;/updated&gt;\n    &lt;!-- author contact information --&gt;\n    &lt;author&gt;&lt;name&gt;John Doe&lt;/name&gt;&lt;email&gt;doe@xyz.org&lt;/email&gt;&lt;/author&gt;\n    &lt;!-- download the pre-defined dataset in GML format in CRS EPSG:25832 --&gt; \n    &lt;entry&gt;\n        &lt;title&gt;soil properties in CRS EPSG:25832 (GML)&lt;/title&gt;\n        &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_25832.gml\" type=\"application/gml+xml;version=3.2\" hreflang=\"en\" length=\"34987\" \n          title=\"soil properties dataset encoded as a GML 3.2 document in ETRS89 UTM zone 32N (http://www.opengis.net/def/crs/EPSG/0/25832)\"/&gt;\n        &lt;id&gt;http://localhost:8080/soilproperties_25832.gml&lt;/id&gt;\n        &lt;updated&gt;2021-06-15T11:12:34Z&lt;/updated&gt;\n        &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/25832\" label=\"ETRS89 / UTM zone 32N\"/&gt;\n    &lt;/entry&gt;\n    &lt;!-- download the same pre-defined dataset in GML format in CRS EPSG:4258--&gt;\n    &lt;entry&gt;\n        &lt;title&gt;soil properties in CRS EPSG:4258 (GML)&lt;/title&gt;\n        &lt;!--file download link--&gt;\n        &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_WGS84.gml\" type=\"application/gml+xml;version=3.2\" hreflang=\"en\" length=\"37762\" \n          title=\"soil properties encoded as a GML 3.2 document in WGS84 geographic coordinates (http://www.opengis.net/def/crs/OGC/1.3/CRS84)\"/&gt;\n        &lt;id&gt;http://localhost:8080/soilproperties_WGS84.gml&lt;/id&gt;\n        &lt;updated&gt;2021-06-14T12:22:09Z&lt;/updated&gt;\n        &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/4258\" label=\"ETRS89\"/&gt;\n    &lt;/entry&gt;\n    &lt;!-- download the same pre-defined dataset in ShapeFile format in CRS EPSG:25832, ShapeFile is in a single zip archive.--&gt;\n    &lt;entry&gt;\n        &lt;title&gt;soil properties in CRS EPSG:25832 (ShapeFile)&lt;/title&gt;\n        &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_25832.zip\" type=\"application/xshapefile\" hreflang=\"en\" length=\"89274\" \n        title=\"soil properties dataset encoded as a ShapeFile in ETRS89 UTM zone 32N (http://www.opengis.net/def/crs/EPSG/0/25832)\"/&gt;\n        &lt;id&gt;http://localhost:8080/soilproperties_25832.zip&lt;/id&gt;\n        &lt;updated&gt;2021-06-15T11:12:34Z&lt;/updated&gt;\n        &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/25832\" \n        label=\"ETRS89 / UTM zone 32N\"/&gt;\n    &lt;/entry&gt;\n&lt;/feed&gt;\n</code></pre> <p>Notice that you can provide multiple distributions for the same dataset (in various projections, translations or formats) to facilitate users.</p> <p>Notice that the examples above incorperate ongoing work as described in https://github.com/INSPIRE-MIF/gp-data-service-linking-simplification/issues/63.</p> <p>The docker container runs locally, so it can not be tested by the INSPIRE Validator.  In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.</p> <p>Note that you have to update the self-references in atom files to use the tunneled web address. Then trigger the Atom validation:</p> <p></p>"},{"location":"tools/webdav/#read-more","title":"Read more","text":"<p>An alternative Atom implementation exists in GeoNetwork. The approach is described at https://geonetwork-opensource.org/manuals/trunk/en/tutorials/inspire/download-atom.html and https://geonetwork-opensource.org/manuals/3.10.x/en/api/opensearch.html. GeoNetwork provides an internal and external mode, the external mode provides opensearch on a set of remote atom feeds. The internal mode generates the atom feeds from metadata records.</p> <p>Hale Studio provides an option to generate a Atom feed while exporting a dataset to GML. The Hale Connect platform offers a Atom based service endpoint for every dataset published.</p> <p>The QGIS INSPIRE Atom plugin provides access to Atom services through QGIS.</p>"},{"location":"tools/xtraserver/","title":"Xtra server","text":"<p>Status: contribution required</p> <p>A proprietary java implementation of WFS, WMS. The package is also distributed as ArcGIS for INSPIRE Classic server extension. The team at interactive-instruments is and has been heavily involved in the definition of GML and the INSPIRE download specifications. They developed ShapeChange which is commonly used to export UML diagrams to XSD. Another product is LDProxy, an open source java implementation of the OGC API suite of standards.</p> <ul> <li>Website: https://www.interactive-instruments.de/en/xtraserver/</li> </ul>"},{"location":"tools/yarrrml/","title":"YARRRML","text":"<p>Status: in progress</p> <p>This recipe provides a quickstart on using YARRRML to convert arbitrary soil data to RDF following the glosis ontology using a YARRRML approach. This work has initially been developed in the scope of the Soils for Africa project.</p> <p>YARRRML is a human readable text-based representation for declarative Linked Data generation rules. YARRRML is encoded in YAML, a widely used data serialization language designed to be human-friendly. YARRRML is an initiative of the RML.io community which provides a set of tools to generate knowledge graphs.</p>"},{"location":"tools/yarrrml/#glosis","title":"Glosis","text":"<p>The glosis ontology has been developed in the scope of the SIEUSOIL project, which aims at implementing and testing a shared China-EU Web Observatory platform that will provide Linked (Open) Data to monitor status and threats of soil and land resources. The GloSIS ontology enables the representation of soil related data in semantic format. The ontology has been derived from the UML GLOSIS data model v1.0, which itself is based on ISO28258 and is similar to the INSPIRE soil model. The ontology has been created in line with best practices and methodologies, reusing existing standard models and ontologies, including sosa/ssn for the representation of observations and measurements, or skos for the representation of codelists.</p>"},{"location":"tools/yarrrml/#matey","title":"Matey","text":"<p>A basic point of entry is the YARRRML editor <code>Matey</code> available online at https://rml.io/yarrrml/matey/. The standard view of the editor has 4 sections - a section for input data - a section to define YARRRML rules - a section to display RDF output - a section to visualize exported RML.io rules from YARRRML</p> <p>YARRRML provides a Quickstart tutorial at https://rml.io/yarrrml/tutorial/getting-started to get acquainted with the interface and YARRRML encoding rules. I recommend to follow the tutorial if you want to proceed after the recipe, for example with your own data.</p>"},{"location":"tools/yarrrml/#soil-data","title":"Soil data","text":"<p>We start our recipe with some typical soil sample analyses data from Africa. Soil samples at location (x_coord,ycoord) at depth (Upper_Depth_Val,Lower_Dept_Val) at a date (datedescribed, YYMM) with a list of observed properties (coSa,meSa,fiSa,vfiSa,coSi,...).</p> <p>Let's load some data into the first panel of Matey. Note that you can switch between data files with the pull down on the top-left of the panel. The pull down also offers the option to <code>create new source</code> or <code>load a remote source</code>. Create a new source and name it <code>soil.csv</code>. Copy the content below in the panel.</p> <pre><code>datedescribed,xcoord,ycoord,SSN,Upper_Depth_Val,Lower_Dept_Val,coSa,meSa,fiSa,vfiSa,coSi,fiSi,Clay,CEC,Carbon,pH_H2O,PH_CaCl2,AWR,Cat_Na,Cat_K,Cat_Ca,Cat_Mg,Micro_Zn,Micro_Mn,Micro_Cu,Micro_Co\n7611,28.36,-25.9533333333333,C3975,0,310,11.2,21.1,38.1,,,7.4,20,4,0.5,5.1,4.4,1.899999976,0,0.100000001,0.400000006,0.100000001,0.189999998,14,1.24000001,0.790000021\n7611,28.36,-25.9533333333333,C3981,0,280,7.8,11.3,21.2,,,12.7,47,11.7,1.5,6.2,5.4,13.5,0.100000001,0.699999988,2.599999905,3.299999952,1.110000014,175,13.10000038,14.01000023\n7611,28.62333333,-25.82666667,C3984,0,180,7.2,32.3,48.2,,,3.5,10,3.7,0.8,5.9,4.9,3,0,0.200000003,0.600000024,0.400000006,0.620000005,4.300000191,0.75999999,0.356999993\n7611,28.62333333,-25.8266666666667,C3986,0,380,10.1,33.7,44.9,,,1.6,9.6,2.8,0.6,5.1,4.3,2.099999905,0,0.100000001,0.100000001,0,0.180000007,2.799999952,0.579999983,0.037999999\n7611,28.62666667,-25.9766666666667,C1488,0,200,12.7,24.2,34.3,,,31,19.6,3.6,0.47,4.9,4.4,2.599999905,0,0.800000012,0.409999996,0.409999996,0.119999997,75,1.450000048,2.25\n7611,28.62666667,-25.9766666666667,C1408,0,250,6.9,15.5,29.1,11.9,4.5,10.4,19.6,4.8,0.84,5.7,4.9,2.299999952,0,0.800000012,1.100000024,0.5,0.310000002,248.5,1.519999981,2.380000114\n7611,28.62666667,-25.9766666666667,C3988,0,370,2.8,5,13.6,,,19.9,54.2,21.4,1.63,6.9,5.9,26.5,0.400000006,1.899999976,10.69999981,6.099999905,0.400000006,229.8999939,6.909999847,15.31000042\n7611,28.85666667,-25.8566666666667,C3991,0,350,2.1,6.5,13.9,7.9,4.8,13.7,51.4,11.7,1.25,6.1,5.4,8.399999619,0.100000001,0.5,4.800000191,3,0.620000005,189.5,4.909999847,18.03000069\n7611,28.85666667,-25.8566666666667,C3995,0,310,7.8,40.4,37.2,,,2.1,12,2.5,0.5,5.4,4.5,2.299999952,0.100000001,0.200000003,0.5,0.300000012,0.159999996,5.25,0.479999989,0.125\n7611,29.12,-25.86,C3998,0,270,1.7,20.6,55.6,,,4.9,16,4.4,0.6,5.6,4.5,5.099999905,0,0.300000012,0.600000024,0.100000001,0.159999996,12,0.850000024,0.43900001\n7205,29.64638889,-25.6211111111111,C1416,0,300,5.4,12,16.8,6.9,6,11.3,49,11.7,2,5.7,4.9,9.5,0.100000001,1.200000048,3.700000048,2,1.139999986,616.0999756,5.059999943,8.899999619\n7208,29.66861111,-25.6905555555556,C1410,0,400,41,34.3,16.3,,,0,5,1.2,0.25,4.8,4.4,2,0,0.100000001,0,0,0.109999999,12.89999962,0.209999993,0.061999999\n7611,28.39444444,-25.4405555555556,C4001,0,300,10.9,28.1,40.4,,,4.3,11.9,3.7,0.8,6.4,5.5,4.900000095,0.100000001,0.400000006,1.200000048,0.699999988,0.550000012,14.30000019,1.309999943,0.610000014\n7611,28.39444444,-25.4405555555556,C4003,0,310,10.7,14.9,39.6,,,5.5,24.2,3.7,0.6,5.3,4.3,2.700000048,0,0.400000006,0.200000003,0.600000024,0.280000001,9.300000191,0.74000001,0.474000007\n7611,28.60194444,-25.4791666666667,C2751,0,300,0.6,2.4,77.7,,,3.7,14.6,4.4,0.4,7,6.1,3.099999905,0.100000001,0.600000024,1.600000024,1,0.219999999,275.2999878,2.279999971,4.599999905\n7611,28.60194444,-25.4791666666667,C2735,0,300,0.8,20.9,62.4,,,2.5,12.9,5.1,0.5,6.9,6.2,2.5,0.100000001,0.699999988,1.799999952,1.600000024,0.970000029,144.1000061,1.149999976,2.059999943\n7312,26.04444444,-29.0941666666667,C2721,0,330,1,12.5,74.2,,,2.4,9.1,4.5,0.4,6.9,6,2.299999952,0.100000001,0.400000006,1.899999976,1.399999976,0.170000002,101.0999985,0.839999974,1.299999952\n7305,26.38194444,-29.0666666666667,C2707,0,450,0.7,2.4,36.3,,,6.9,51.3,23.1,1,8.7,7.6,166.5,0.800000012,0.300000012,20.39999962,5.300000191,0.200000003,276.2000122,3.339999914,3.450000048\n7305,26.38333333,-29.125,C2718,0,350,0.9,9,72.1,,,3.5,14.2,4,0.4,7.1,6.2,1.799999952,0.100000001,0.600000024,1.899999976,1.5,0.340000004,176.6999969,1.389999986,2.200000048\n7305,26.29111111,-29.2755555555556,C2715,0,350,2,5.5,72.3,,,3.3,15.6,4.8,0.6,6.7,5.9,3.900000095,0.100000001,0.800000012,2.299999952,1.399999976,0.159999996,113.4000015,1.600000024,1.50999999\n7305,26.3525,-29.3083333333333,C2739,0,150,1.1,2.9,64.4,,,7.1,24,9.5,1,6.9,6.3,6,0.200000003,1,4.099999905,3.200000048,0.370000005,181.1999969,2.819999933,2.680000067\n7212,26.04111111,-29.9866666666667,C2883,0,200,0.2,1.4,66.9,,,10.1,16.3,5.7,0.5,6.7,5.9,2.400000095,0.100000001,0.699999988,2.400000095,1.399999976,1.419999957,200.1999969,1.639999986,2.400000095\n7212,26.03055556,-29.9963888888889,C2872,0,150,0.3,1.2,38.9,34.9,8.6,4.3,10.7,4.6,0.3,8.1,7.3,2.299999952,0.100000001,0.400000006,5.5,1,0.340000004,138.5,1.470000029,2.230000019\n7303,26.81861111,-29.7008333333333,C2842,0,250,0.9,4.2,48,21.7,8.3,4.8,11.7,4.3,0.4,6.2,5,3.400000095,0.100000001,0.400000006,1.799999952,1.399999976,0.5,106.6999969,1.059999943,1.139999986\n7303,26.76222222,-29.6647222222222,C2845,0,280,0.2,1.3,35.9,34.6,8.2,5.6,12.1,5.1,0.6,6.8,5.7,3.200000048,0.100000001,0.699999988,2,1.200000048,1.429999948,164,2.019999981,1.340000033\n7207,26.57527778,-29.4588888888889,C1991,0,400,0.5,0.4,37,,,12.5,50.3,25.2,1.2,7.1,6.3,,0.699999988,0.5,19.70000076,6.900000095,0.469999999,362.3999939,3.819999933,4.300000191\n7207,26.67388889,-29.4505555555556,C1993,0,250,4.6,1.6,57.8,,,15.7,20.3,8.9,1,6,5.4,3.700000048,0.300000012,0.600000024,3.299999952,2.099999905,0.870000005,234.6000061,2.579999924,3.220000029\n7207,26.77722222,-29.3152777777778,C1989,0,300,0.3,2.9,69.2,,,10.7,18.9,7.3,0.7,6.4,5.4,2.799999952,0.200000003,0.400000006,3.700000048,2.099999905,0.419999987,73.5,1.809999943,1.870000005\n7207,26.78,-29.1319444444444,C1982,0,300,1.6,1.2,68.4,,,11.1,16,5.3,0.6,6.7,5.6,3.5,0.100000001,0.5,2.599999905,1.299999952,0.600000024,225.6000061,2.49000001,1.970000029\n7207,26.92222222,-29.1233333333333,C1954,0,300,1.8,0.8,54.5,,,16,21.3,8,0.65,5.9,4.6,8.100000381,0.400000006,0.200000003,2.799999952,1.899999976,0.200000003,222.6000061,2.180000067,2.640000105\n7207,27.14,-29.0133333333333,C1972,0,300,2.9,2.7,72.9,,,9.5,8.4,4.5,0.5,6.6,5.7,2.700000048,0.100000001,0.5,2.099999905,1,0.300000012,124.8000031,1.659999967,1.580000043\n7207,27.02444444,-29.2325,C1995,0,350,1.4,2.6,65.7,,,9.2,16.4,5.7,0.5,5.7,5.3,2.5,0.100000001,0.400000006,3.099999905,1,0.219999999,133.1000061,2.359999895,1.350000024\n7207,27.01666667,-29.2647222222222,C2001,0,300,0.2,3,34.5,25.3,14.2,8.1,12.2,4.5,0.5,5.7,5.2,22.29999924,0.100000001,0.100000001,2.200000048,1.200000048,0.49000001,57.29999924,2.089999914,1.019999981\n7207,26.99055556,-29.3361111111111,C2006,0,300,0.8,2,78,,,5.6,10.2,4,0.5,5.9,5.3,72.38,0.100000001,0.300000012,2.200000048,0.800000012,0.439999998,123.3000031,2.059999943,2.079999924\n7207,26.99083333,-29.3447222222222,C2004,0,280,0.8,3.4,70.5,,,10.4,12.3,5.7,0.7,5.7,5.2,28,0.200000003,0.300000012,2.5,1.600000024,0.419999987,108.3000031,2.140000105,1.919999957\n7207,27.03333333,-29.4180555555556,C1998,0,250,2,1,54.8,,,22.4,16.5,5.7,0.8,5.8,5.3,1.700000048,0.100000001,0.600000024,2.299999952,1.200000048,1.139999986,300.7999878,2.599999905,1.649999976\n7406,24.1025,-28.9644444444444,C3315,0,300,3.4,25,65.3,,,0.4,5.8,2.7,0.9,7.2,6.4,1.700000048,0,0.200000003,1.600000024,0.899999976,0.02,55.5,0.25,0.660000026\n7406,24.20194444,-28.9486111111111,C3320,0,400,2.9,28.6,61.9,,,0.3,4.6,1.8,0.1,7.2,6.4,1.600000024,0,0.200000003,1.200000048,0.600000024,0.114770003,55.5,0.229509994,0.680000007\n7406,24.62638889,-28.9102777777778,C3318,0,250,2.7,9.6,77.5,,,3.3,6.1,4.6,0.2,7,6.2,2.299999952,0.100000001,0.5,2.599999905,1.100000024,0.059999999,88.69999695,0.920000017,2.279999971\n7406,24.86027778,-28.6508333333333,C3323,0,350,3.4,29.6,60.4,,,0.1,6.3,2.9,0.1,7.4,6.5,2.5,0.100000001,0.200000003,1.200000048,1.200000048,0.029999999,89.30000305,0.370000005,0.980000019\n7406,24.46666667,-28.5133333333333,C3313,0,300,5.6,15,64.4,,,4.2,10.5,4.6,0.2,7.4,6.5,2.299999952,0,0.300000012,3.5,0.800000012,0.479999989,105.8000031,1.74000001,5.039999962\n7406,24.48055556,-28.4327777777778,C3308,0,300,9.8,19.2,59.8,,,2.5,7.4,3.9,0.2,8,7.1,2.599999905,0,0.200000003,3.200000048,0.5,0.039999999,201.5,1.24000001,3.190000057\n7302,25.33694444,-28.0905555555556,C2813,0,420,0.5,19.9,73,,,0.9,4.5,2,0.2,6.6,5.7,1.100000024,0.100000001,0.400000006,1.100000024,0.5,0.090000004,70.30000305,0.400000006,0.493420005\n7302,25.47361111,-28.1827777777778,C2804,0,350,3.9,29,55.8,,,1.6,9.3,4.5,0.3,7.5,6.6,1.299999952,0.100000001,0.5,2.900000095,1.700000048,0.100000001,78.90000153,1.120000005,3.059999943\n7302,25.47777778,-28.2077777777778,C2807,0,350,0.7,22.9,68.8,,,0.5,5,1.7,0.1,6.7,6,1.399999976,0.100000001,0.400000006,1.100000024,0.899999976,0.140000001,60.79999924,0.360000014,0.680000007\n7302,25.41722222,-28.225,C2810,0,350,1.4,26.8,60.9,,,1.4,7.4,3.2,0.3,7.6,7,1.700000048,0.100000001,0.600000024,3,1.100000024,0.219999999,105.0999985,0.439999998,1.5\n7306,25.92277778,-28.8738888888889,C2748,0,300,0.8,11.1,61.2,,,5.4,20.1,7.5,0.6,7,6.5,5.5,0.100000001,0.400000006,4.400000095,2.099999905,0.150000006,149.3999939,1.960000038,2.25\n7302,25.89444444,-28.7016666666667,C2793,0,350,0.6,15.5,76.7,,,0.5,7.9,3,0.2,7.3,6.4,1.299999952,0.100000001,0.300000012,1.899999976,0.800000012,0.239999995,11.60000038,0.589999974,1.25\n7302,25.51666667,-28.5672222222222,C2795,0,320,1,7.9,66.3,,,5.4,17.8,8.1,0.6,6.8,5.9,3.900000095,0.100000001,0.400000006,4.900000095,3,0.300000012,378.1000061,3.450000048,5.019999981\n7302,25.59,-28.5072222222222,C2819,0,400,0.9,23.2,67.7,,,0.9,7.5,2.8,0.2,6.8,5.6,1.299999952,0.100000001,0.5,1.600000024,0.800000012,0.189999998,129.1000061,0.460000008,0.860000014\n</code></pre> <p>In the second panel we're going to write our YARRRML rules. We start by defining the GLOSIS prefixes.</p> <pre><code>prefixes:\n glosis_lh: \"http://w3id.org/glosis/model/layerhorizon#\"\n iso28258: \"http://w3id.org/glosis/model/iso28258/2013#\"\n sosa:  \"http://www.w3.org/ns/sosa/\"\n s4a: \"http://data.soils4africa-h2020.eu#\"  \n</code></pre> <p>Then continue with the mapping of horizons. Add the section below under the prefices section. Then click <code>Generate LD</code> and notice some RDF is shown representing our horizons in Turtle RDF.</p> <pre><code>mappings:\n  horizon:\n    sources:\n      - ['soil.csv~csv']\n    s: http://data.soils4africa-h2020.eu/horizon#$(SSN)\n    po:\n      - [a, glosis_lh:GL_Horizon]\n      - [iso28258:upperDepth, $(Upper_Depth_Val)]\n      - [iso28258:lowerDepth, $(Lower_Dept_Val)] \n</code></pre> <p>Notice how for each record in the csv a GL_Horizon entity is created with a unique identification and an upper and lower property.</p> <p>Next are the actual observations. Unfortunately our data does not provide any information on the procedures used to produce the measurement result. We start with mapping the Cation Exchange Capacity.</p> <pre><code>  observation:\n    sources:\n       - ['soil.csv~csv']\n    s: http://data.soils4africa-h2020.eu/observation/CEC#$(SSN)\n    po:\n      - [a, glosis_lh:EffectiveCec]\n      - [sosa:observedProperty, glosis_lh:EffectiveCecProperty]\n      - [sosa:hasResult,  $(CEC)]\n      - p: sosa:hasFeatureOfInterest\n        o: \n          mapping: horizon\n          condition:\n            function: equal\n            parameters:\n              - [str1, $(CEC), s]\n              - [str2, $(CEC), o]\n</code></pre> <p>You can extend the mapping by adding additional rules.</p>"},{"location":"tools/yarrrml/#publishing-rdf-into-a-triple-store","title":"Publishing RDF into a triple store","text":"<p>A typical mechanism to publish RDF generated with YARRRML to a triple store is to convert the YARRRML rules to RML rules and use these in a data workflow in which data is transformed and loaded on a triplestore in an automated way. For example with the RMLMapper or RMLStreamer tool which can be triggered as a docker image for example in a workflow step.</p> <pre><code>docker run -v $PWD:/data --rm rmlio/rmlstreamer toFile -m /data/mapping.ttl -o /data/output\n</code></pre> <p>An interesting initiative using this approach is the data to services pipeline project by Maastricht University.</p>"},{"location":"tools/zenodo/","title":"Zenodo","text":""},{"location":"utils/docker/","title":"Overview Docker","text":"<p>Docker is a virtualisation technology slightly more efficient then running a virtual machine. With docker you run a full virtual environment (container) within your PC. Most containers run a flavour of Linux and you access them as if you access a remote server. Containers run an <code>docker-image</code>, a prepared set of operating system and software. Docker images are build locally from a <code>Dockerfile</code> (recipe) or downloaded from a repository such as <code>dockerhub</code>. Learn more about docker in the Docker Overview.</p> <p>In this recipe we run most examples using Docker, because there is no need to install any software on your computer, which is either not allowed, give errors due to missing dependencies or in a worse case scenario can corrupt an existing program. </p> <p>On Windows and Apple we recommend to install Docker Desktop. Docker Desktop provides an additional panel to manage running containers. On Linux you can install docker engine. Start docker engine from the start menu, if it is not already running.</p> <p>In this recipe we will use the following Docker concepts:</p> <ul> <li>Docker container; an image deployed into a virtual environment. Most containers usually have an assigned port, so you can access the service of the container via the browser (for example, http://localhost:5000). You can also interact with containers from your commandline (docker ps, docker logs xxx, docker run geopython/pygeoapi)</li> <li>Docker volume; a folder on the host system which you mount as a folder into the container. Containers are destroyed when stopped, any file stored in the container file system is lost. By mounting a host folder into the container, you can persist items between runs of the container.</li> <li>Docker compose; with a compose file you arrange a cluster of containers into a functional system. One container runs a database, another container runs a webserver and the third container runs the web application. The compose file arranges that the containers can connect, on which port they run and which volumes they load.</li> <li>Docker build is the command to build a docker image from a Dockerfile (a docker recipe)</li> <li>With Docker pull you can pull an image from a repository </li> <li>Docker run starts a container, press ctrl-c to stop is again (unless you run it with -d option) </li> </ul> <p>Instead of running containers for permanent server applications, you can also start a container to run a short process, similar to running a command line utility. The container will stop when the process is finished. </p>"},{"location":"utils/docker/#docker-exercise","title":"Docker exercise","text":"<ul> <li>Install Docker Desktop, verify it is running, else start it from the start menu.</li> <li>On commandline run this command</li> </ul> <p><code>docker run -p80:80 uzyexe/tetris:latest</code></p> <ul> <li>Open browser at http://localhost</li> <li>in command line ctrl-c to stop the container</li> </ul> <p>(if other processes are running on port 80, Docker will throw an error, select another port, e.g. -p81:80, and open http://localhost:81)</p>"},{"location":"utils/git/","title":"Overview GIT","text":"<p>GIT is a distributed version management system of mainly text files to facilitate shared development of software and/or documentation. In GIT everybody <code>checks out</code> the full repository with all its history, you make changes to a version locally and <code>push</code> them back to the server. In the process incidental <code>conflicts</code> may occur, if someone else has made a change in the same version and line as you. The server will reject the push until you resolved the conflict.</p> <p>GIT is mainly operated from the commandline, but a lot of client software is available to facilitate participation. An example is SmartGIT, which provides an easy to use interface to manage even complex git tasks such as fixing conflicts. </p> <p>Github.com is a well known provider of GIT services. They offer a lot of additional functionality on top of the GIT version management, such as a web interface, issue management, forking &amp; pull requests, wiki, actions, etc. </p> <p>Gitlab is a popular open source software stack offering a similar set of functionality as Github, you can use it as SAAS or install it on a server. </p> <p>An interesting quick start to GIT is written by Roger Dudler (multiple translations). Or if you prefer a video.</p>"},{"location":"utils/git/#git-use-cases","title":"GIT Use Cases","text":"<ul> <li>A software project with contributors from around the globe. Including issue management, software releases and automated validation of unit and integration tests.</li> <li>This wiki is maintained in a GIT repository. With every new push a new version is build and it replaces the previous version.</li> <li>A roll out of an improved composition of Docker containers on a cloud platform like Kubernetes. The helm charts (configuration files) are stored on a GIT repository, every change to the helm chart results in an update of the development environment to reflect the latest changes.</li> <li>GIT is an important tool for Dev Ops staff. Manual interventions are minimized. Every action is scripted, stored in a GIT repository and started from a GIT action. This improves tracebility and reproducability in software maintenance.</li> </ul>"},{"location":"utils/git/#exersize","title":"Exersize","text":"<ul> <li>Install git (or smartgit, which has git included) on your machine </li> <li>Clone the repository of this wiki from the command line:</li> </ul> <pre><code>git clone https://github.com/ejpsoil/soildata-assimilation-guidance.git\n</code></pre> <ul> <li>open the folder and log all events that happened on the repository </li> </ul> <pre><code>cd soildata-assimilation-guidance\ngit log --oneline\n</code></pre>"},{"location":"utils/git/#forking-and-pull-requests","title":"Forking and Pull requests","text":"<p>Github added an extra interactivity on top of GIT, the option to <code>Fork</code> a repository to a personal space. And from the personal space provide an option to propose a change to the <code>upstream</code> repository, this is called a <code>pull request</code>. Now users were able to propose changes to repositories of which they were not even a member yet. The fork and pull mechanism has now been adopted by other git platforms, such as Gitlab, but because it is not part of the GIT specification itself, it may work slightly different on other platforms. Let's try and improve this wiki via a pull request:</p> <ul> <li>If you do not have a github account yet, we invite you to set up one now at https://github.com/signup.</li> <li>When logged in, open https://github.com/ejpsoil/soildata-assimilation-guidance, and click the fork button in the top right. This will clone the repo to your personal space.</li> <li>On https://github.com/YOU/soildata-assimilation-guidance, edit a file (maybe you found a typo somewhere, or would like to comment/extend something) via the github web interface by opening the file and clicking on the pencil button.</li> <li>Below the text you notice a commit message and button. Every commit in GIT requires a usefull message, so others understand what you did. Select the option \"Create a new branch for this commit ...\".  </li> <li>Then create the branch, but not the pull request in the next step, because this pull request would arrive in your own repository</li> <li>Now navigate back to https://github.com/ejpsoil/soildata-assimilation-guidance. A banner shows indicating you can <code>compare and pull request</code> your recent change. Click this button and create the pull request.</li> </ul> <p>The process of a pull request is quite overwhelming. But it is an important aspect in collaborative development these days. Many repositories only allow code changes via pull requests, because it is a guarantee that at least 2 persons have looked at it.</p>"},{"location":"utils/jupyter/","title":"Jupyter notebooks","text":"<p>Status: in progress</p> <p>Jupyter notebooks are a combination of text and code (JUlia, PYThon and R), the code can directly be run within the notebook. Jupiter notebooks are mainly used in training or documentation settings in data science. Some argue that jupyter notebooks will replace articles in scientific magazines to communicate research results.</p> <p>Some examples of Jupyter notebooks of interest:</p> <ul> <li>https://inspire.rasdaman.org/apps/jupyter-notebook/index.html</li> <li>https://github.com/geopython/geopython-workshop</li> </ul> <p>A Quick Start on using Jupyter is written by Antonino Ingargiola.</p>"},{"location":"utils/jupyter/#exercise","title":"Exercise","text":""},{"location":"utils/jupyter/#read-more","title":"Read more","text":"<ul> <li>Website: http://jupyter.org/</li> <li>Documentation: https://docs.jupyter.org/en/latest/</li> <li>Wikipedia: https://en.wikipedia.org/wiki/Project_Jupyter</li> </ul>"},{"location":"utils/localtunnel/","title":"Overview Local Tunnel","text":"<p>This recipe describes an approach to temporarily host a local webservice as an online service. A utility opens a tunnel to a service provicer, the service provider routes all traffic for a specific domain via the tunnel to your machine. The tunnel stops if you quit the utility (ctrl-c).</p> <p>Various (free) service providers exist offering this service:</p> <ul> <li>localtunnel requires nodejs </li> <li>ngrok web based, but requires registration </li> <li>localhost.run requires SSH to be installed</li> </ul>"},{"location":"utils/localtunnel/#exercise","title":"Exercise","text":"<ul> <li>Verify a docker image is running, for example: </li> </ul> <p><code>docker run -p80:80 -d uzyexe/tetris:latest</code></p> <ul> <li>Enter this command:</li> </ul> <p><code>ssh -R 80:localhost:80 nokey@localhost.run</code></p> <ul> <li>The utility will display a url on which the service will be available, try this url in your browser (and phone, to make sure it works also outside your computer)</li> </ul>"},{"location":"utils/visualstudiocode/","title":"Overview Text Editors","text":"<p>When working with a variety of text files (html, csv, yaml, xml, geojson, js, python, markdown), you need a replacement for the basic Notepad. A magnitude of options exits, such as VIM, Notepad++, PyCharm, Eclipse, Sublime. The last years my personal favourite has become Visual Studio Code. But let's try to stay neutral and list what features we expect from a daily used text editor:</p> <ul> <li><code>Find in files</code> (and replace) is an important feature when looking for a certain pattern in a folder of files. </li> <li>Syntax highlighting for xml, json, python and yml facilitate readability of the file</li> <li>Code formatting/validation. In python and yaml indenting is essential, that's when code formatting is extra important.</li> <li>A tree view of the project structure, so you can easily open files from the project </li> <li>Code completion/suggestions when you start typing</li> </ul> <p>Optional features:</p> <ul> <li>Preview HTML &amp; Markdown</li> <li>XML schema validation</li> <li>GIT operations</li> <li>Content comparison, compare 2 (or more) files.</li> </ul> <p>Many of the text editor communities have a range of plugins available to extend the functionality of the editor. Various text editors have for example a Mapserver Mapfile plugin, which facilitates mapserver mapfile development. </p>"}]}